{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Rapids\n",
    "\n",
    "<img src=\"./nb_images/rapids.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Rapids is a data preparation and machine learning library that is designed to take maximum advantage of the Nvidia GPU.  The libraries are called cuDF and cuML and take a lot of the same design and API semantics from Pandas and Sklearn python libaries.   Speedups of over 10x are not uncommon for a lot of everyday tasks.\n",
    "\n",
    "If you are familiar with Pandas and Sklearn, this code in this lab will look familiar, but if not thats ok too.  Rapids is still under development, so it is not as full featured as the Pandas and Sklearn libraries, but it is continually getting new functions.  \n",
    "\n",
    "The following lab will walk you through how to use Rapids with a sample dataset.  **This lab will focus on the performance capabilities of RAPIDS by comparing it to Pandas and Sklearn equivalent operations.** It is not meant to be a machine learning tutorial. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:30:19.593186Z",
     "start_time": "2019-09-18T14:30:19.590705Z"
    }
   },
   "source": [
    "## A word on performance comparisons of RAPIDS vs Pandas\n",
    "\n",
    "Pandas and Numpy are two of the most popular libraries for both data engineers and data scientists.  The libraries are very robust and perfomant, but one major drawback is that they are single threaded libraries.  When comparing RAPIDs vs Pandas/Numpy you are seeing the benefit of parallelizing these types of tasks overs potentially thousands of seperate threads.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuDF basics\n",
    "\n",
    "Built based on the Apache Arrow columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and manipulating data.\n",
    "\n",
    "cuDF provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming.\n",
    "\n",
    "Definitions :\n",
    "* GPU Dataframe : a dataframe from the RAPIDS cuDF library running on the GPU\n",
    "* Apache Arrow  : common columnar in memory data format project\n",
    "* Pandas        : data preparation and engineering library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Execute the functions below, they are needed for follow-on parts of the lab.  Note the **pgdf** function is a convenience function to display the GPU dataframe in a nice format for jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:30.835569Z",
     "start_time": "2019-10-03T14:04:29.272710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../utils/') \n",
    "\n",
    "#dask\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "\n",
    "# Rapids\n",
    "import cudf\n",
    "from cudf.dataframe import DataFrame as RapidsDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:30.840396Z",
     "start_time": "2019-10-03T14:04:30.837572Z"
    }
   },
   "outputs": [],
   "source": [
    "# [print gpu dataframe] helper function to print GPU dataframes \n",
    "def pgdf(gdf) :\n",
    "    display(gdf.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:31.075167Z",
     "start_time": "2019-10-03T14:04:30.842230Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_command(cmd,repeat=1) :\n",
    "    avg_runtime = timeit.timeit(cmd, number=repeat)\n",
    "    return float(avg_runtime / repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:31.223048Z",
     "start_time": "2019-10-03T14:04:31.076871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary to store results ..\n",
    "# example \"describe\" : {\"gpu\" : []}\n",
    "# TODO : make display results look better ..\n",
    "class COMPARE() :\n",
    "        ## Abstract Custom Implementations\n",
    "    def __init__(self) :\n",
    "        #nprint(\"Loading Data.  Overriding __init__ from dfutils\")\n",
    "        self.tests = []\n",
    "        self.gpu_results = {}\n",
    "        self.cpu_results = {}\n",
    "        self.df_shape = (0,0)\n",
    "        self.df_memory_gb = 0 \n",
    "\n",
    "    def add_result(self, test_name, gpu_result, runtime) :\n",
    "        if test_name not in self.tests :\n",
    "            self.tests.append(test_name)\n",
    "            self.gpu_results[test_name] = []\n",
    "            self.cpu_results[test_name] = []\n",
    "        \n",
    "        if(gpu_result == \"gpu\") :\n",
    "            self.gpu_results[test_name].append(runtime)\n",
    "        else :\n",
    "            self.cpu_results[test_name].append(runtime)\n",
    "            \n",
    "    def display_results(self) :\n",
    "        print(\"Dataframe size : {} {} GB\".format(self.df_shape, self.df_memory_gb))\n",
    "        print(\"{:<20} {:<20} {:<20} {:<20}\".format(\"test\", \"CPU(s)\", \"GPU(s)\", \"GPU Speedup\"))\n",
    "        for i in self.tests :\n",
    "            cpu_mean = sum(self.cpu_results[i]) / (len(self.cpu_results[i])+0.00001)\n",
    "            gpu_mean = sum(self.gpu_results[i]) / (len(self.gpu_results[i])+0.00001)\n",
    "            su = cpu_mean / (gpu_mean + .00001)\n",
    "            print(\"{:<20} {:<20.4f} {:<20.4f} {:<20.2f}\".format(i, cpu_mean, gpu_mean, su ))\n",
    "\n",
    "run_times = COMPARE()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:31.463306Z",
     "start_time": "2019-10-03T14:04:31.454907Z"
    }
   },
   "outputs": [],
   "source": [
    "def pca_scree(pca_explained_variance, label) :\n",
    "        \n",
    "    # bin is my x axis variable\n",
    "    bin = []\n",
    "    for i in range (len(pca_explained_variance)):\n",
    "        bin.append(i+1)\n",
    "    # plot the cummulative variance against the index of PCA\n",
    "    cum_var = np.cumsum(pca_explained_variance)\n",
    "    plt.plot(bin, cum_var)\n",
    "    # plot the 95% threshold, so we can read off count of principal components that matter\n",
    "    plt.plot(bin, [.95]*n_components, '--')\n",
    "    plt.plot(bin, [.75]*n_components, '--')\n",
    "    plt.plot(bin, [.50]*n_components, '--')\n",
    "    #turn on grid to make graph reading easier\n",
    "    plt.grid(True)\n",
    "    #plt.rcParams.update({'font.size': 24})\n",
    "    plt.suptitle(label + ' PCA Variance Explained')\n",
    "    plt.xlabel('Number of PCA Components', fontsize=18)\n",
    "    plt.ylabel('Fraction of Variance \\nExplained', fontsize=16)\n",
    "    # control number of tick marks, \n",
    "    plt.xticks([i for i in range(0,n_components)])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T14:10:21.285474Z",
     "start_time": "2019-08-26T14:10:21.282708Z"
    }
   },
   "source": [
    "### Useful DataFrame attributes\n",
    "\n",
    "When you create a GPU dataframe, there are a number of methods available for you to understand the composition.  The detailed list is found in the Rapids [cuDF documentation](https://docs.rapids.ai/api/cudf/0.7/) \n",
    "\n",
    "Below we will create a small cuDF dataframe and look at some of its attributes.  A few of these attributes come in handy when debugging \n",
    "\n",
    "* dtypes  :  Shows all the columns and associated data types \n",
    "* shape   :  Shows the shape (rows / columns) of the dataframe\n",
    "* columns :  Show the column names in a python list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:33.022177Z",
     "start_time": "2019-10-03T14:04:32.792439Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a simple GPU dataframe\n",
    "df = cudf.DataFrame()\n",
    "df['column1'] = [0, 1, 2, 3, 4]\n",
    "df['column2'] = [float(i + 10) for i in range(5)]  # insert column\n",
    "df['column3'] = [\"bbb\",\"aaa\",\"ccc\",\"eee\",\"Ddd\"]  # insert column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:34.080375Z",
     "start_time": "2019-10-03T14:04:34.058645Z"
    }
   },
   "outputs": [],
   "source": [
    "#Print the dataframe\n",
    "pgdf(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:35.393881Z",
     "start_time": "2019-10-03T14:04:35.386801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe attributes\n",
    "print(\"\\nDataframe datatypes\\n---------------------\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nDataframe Shape\\n---------------------\")\n",
    "print(df.shape)\n",
    "print(\"\\nDataframe dimensions\\n---------------------\")\n",
    "print(df.ndim)\n",
    "print(\"\\nDataframe Column names\\n---------------------\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:05:34.555877Z",
     "start_time": "2019-09-18T14:05:34.553373Z"
    }
   },
   "source": [
    "### Create a cuDF dataframe from Numpy/Pandas array\n",
    "Rapids cuDF supports the conversion of pandas and numpy arrays to cuDF dataframes.  In the example below we show examples of how you can do this for each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:37.790423Z",
     "start_time": "2019-10-03T14:04:36.832547Z"
    }
   },
   "outputs": [],
   "source": [
    "# Numpy array to cuDF\n",
    "# Dataframe Operations : Create random large array 100x100\n",
    "a = np.random.rand(100,100)\n",
    "df_np = cudf.DataFrame()\n",
    "df_np = df.from_records(a)\n",
    "#df['random_column1'] = [0, 1, 2, 3, 4]\n",
    "pgdf(df_np.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:40.207353Z",
     "start_time": "2019-10-03T14:04:39.906489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas Example\n",
    "pdf = pd.DataFrame({'a': [0, 1, 2, 3],'b': [0.1, 0.2, None, 0.3]})\n",
    "df = cudf.from_pandas(pdf)\n",
    "pgdf(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Operations : Slice  Example - grab 3 arbitrary columns\n",
    "\n",
    "Sometimes you want to grab slices of dataframes.  Here you can just pass a list of column names to the GPU dataframe to return the columns you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:41.511044Z",
     "start_time": "2019-10-03T14:04:41.480765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the df_np array we created above and grab columns 0,1, and 5\n",
    "new_df = df_np[[0,1,5]] \n",
    "pgdf(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:14:36.327434Z",
     "start_time": "2019-09-18T14:14:36.325191Z"
    }
   },
   "source": [
    "### Optional Exercise : Create a Random numpy array 1000 x 1000 and then convert to GPU dataframe.  The select columns 444,555,888 from the array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuML basics\n",
    "\n",
    "CuML is the a machine learning library implemented on the Nvidia GPU.  This allows you to use many of the most common machine learning algorithms without having to write CUDA code.  The list of algorithms is growing with each release so its worth taking a look at the cuML github repo, but in general you can expect a 10x to 50x performance speedup when using the GPU enabled algorithm.  **Later in this lab we will see examples of PCA and linear regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "<img src=\"./nb_images/dask.png\" alt=\"Drawing\" style=\"float :left; margin-right: 20px; width: 200px;\" />\n",
    "\n",
    "\n",
    "Dask is an extremely useful python library that enables parallel execution of arbitrary python programs allowing you to make maximum use of system resources.  It is typically used for libraries that are written in single threaded implementation like pandas and numpy, but its also very useful for running many a parallel tasks when using RAPIDS.  We will have a code sample to demonstrate this at the end of the lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Use Case \n",
    "\n",
    "**The main goal of this lab is to focus on the performance differences of Rapids(GPU) vs Pandas/Sklearn (CPU) implementations.**  \n",
    "<br>\n",
    "\n",
    "<img src=\"./nb_images/lendingclub.png\" alt=\"Drawing\" style=\"float :left; margin-right: 20px; width: 300px;\" />\n",
    "<br>To do this we will use the Lending Club publicly available dataset. \n",
    "This data set is published by lending club and contains information regarding prospective loan applicants.    \n",
    "<br><br><br><br>\n",
    "\n",
    "**As we go through the lab, we will show the similarity in the syntax/usage of the RAPIDS library using this real world dataset and keep track of the runtimes in a comparison report.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T16:25:07.742462Z",
     "start_time": "2019-08-23T16:25:07.740235Z"
    }
   },
   "source": [
    "# Lending Club data and Lab Details\n",
    "\n",
    "Here we will load in the lending club dataset and perform some basic data preparation steps.  \n",
    "\n",
    "Each section is composed of the same workflow\n",
    "\n",
    "- timed cpu example\n",
    "- timed gpu example\n",
    "- comparison of results\n",
    "- logging of runtimes into a comparison table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load the Lending Club Data\n",
    "\n",
    "Here we will load the data twice.  Once into a pandas dataframe **loan_pdf** and once into a rapids dataframe **loan_rdf**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:45.055260Z",
     "start_time": "2019-10-03T14:04:44.655792Z"
    }
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "filename = \"../dataprep_common/loan_project_df.parquet.gzip\"\n",
    "\n",
    "# Expand data to highlight performance difference\n",
    "# 3 ~ 1GB dataset\n",
    "# 4 ~ 2GB dataset \n",
    "# ... etc\n",
    "DATA_DOUBLE_FACTOR=3\n",
    "\n",
    "# Pandas dataframe\n",
    "loan_pdf = pd.read_parquet(filename)#  , names=ts_cols,dtype=ts_dtypes,skiprows=1)\n",
    "\n",
    "# Rapids Dataframe\n",
    "loan_rdf = cudf.read_parquet(filename)#  , names=ts_cols,dtype=ts_dtypes,skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:47.635991Z",
     "start_time": "2019-10-03T14:04:45.066639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale up data to 10 million rows \n",
    "for i in range(DATA_DOUBLE_FACTOR) :\n",
    "    loan_pdf = pd.concat([loan_pdf,loan_pdf],axis=0)\n",
    "    loan_rdf = cudf.concat([loan_rdf,loan_rdf],axis=0)\n",
    "    loan_rdf = loan_rdf.reset_index().drop(\"index\",axis=1)\n",
    "    loan_pdf = loan_pdf.reset_index().drop(\"index\",axis=1)\n",
    "    #pgdf(loan_rdf.head())\n",
    "    #display(loan_pdf.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T15:24:35.673672Z",
     "start_time": "2019-09-24T15:24:35.669534Z"
    }
   },
   "source": [
    "### RAPIDS Dataframe attributes\n",
    "Take a look at both the Rapids dataframe and Pandas dataframe printouts below and convince yourself these are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:47.645768Z",
     "start_time": "2019-10-03T14:04:47.637842Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe attributes\n",
    "print(\"Rapids\")\n",
    "print(\"\\nDataframe datatypes\\n---------------------\")\n",
    "print(loan_rdf.dtypes)\n",
    "print(\"\\nDataframe Shape (rows,cols)\\n---------------------\")\n",
    "print(loan_rdf.shape)\n",
    "print(\"\\nDataframe dimensions\\n---------------------\")\n",
    "print(loan_rdf.ndim)\n",
    "print(\"\\nDataframe Column names\\n---------------------\")\n",
    "print(loan_rdf.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T15:25:07.038547Z",
     "start_time": "2019-09-24T15:25:07.034741Z"
    }
   },
   "source": [
    "### Pandas Dataframe attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:47.836177Z",
     "start_time": "2019-10-03T14:04:47.647622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe attributes\n",
    "print(\"\\n\\nPandas\")\n",
    "print(\"\\nDataframe datatypes\\n---------------------\")\n",
    "print(loan_pdf.dtypes)\n",
    "print(\"\\nDataframe Shape (rows,cols)\\n---------------------\")\n",
    "print(loan_pdf.shape)\n",
    "print(\"\\nDataframe dimensions\\n---------------------\")\n",
    "print(loan_pdf.ndim)\n",
    "print(\"\\nDataframe Column names\\n---------------------\")\n",
    "print(loan_pdf.columns)\n",
    "print(\"\\nDataframe Memory Usage\\n---------------------\")\n",
    "print(loan_pdf.memory_usage(index=True).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T11:19:53.404655Z",
     "start_time": "2019-09-23T11:19:53.402386Z"
    }
   },
   "source": [
    "### Inspect DataSet and Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:49.598249Z",
     "start_time": "2019-10-03T14:04:49.592522Z"
    }
   },
   "outputs": [],
   "source": [
    "run_times.df_shape = loan_pdf.shape\n",
    "run_times.df_memory_gb = loan_pdf.memory_usage(index=True).sum() /10**9\n",
    "\n",
    "print(\"Initial Data set size ~= {:0.3f} GB for this experiment\".format(run_times.df_memory_gb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T14:04:52.420542Z",
     "start_time": "2019-10-03T14:04:50.344399Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out a small sample of the dataframe\n",
    "pgdf(loan_rdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T19:36:33.650293Z",
     "start_time": "2019-08-26T19:36:33.647835Z"
    }
   },
   "source": [
    "## Descriptive Statistics - Describe Performance comparison\n",
    "\n",
    "The first comparison we will make is using the describe function.  Describe is useful because it looks at all the descriptive statistics of the dataset.  It calculates **mean/standard deviation/medain statistics** for all the numerical columns.  If you have a large dataframe it can take some time to calculate.  Lets see how Rapids performs  with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:09:43.235325Z",
     "start_time": "2019-09-24T20:09:38.254566Z"
    }
   },
   "outputs": [],
   "source": [
    "# CPU / pandas\n",
    "loan_pdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:09:43.647178Z",
     "start_time": "2019-09-24T20:09:43.238139Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU / Rapids\n",
    "pgdf(loan_rdf.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:09:48.849367Z",
     "start_time": "2019-09-24T20:09:43.648929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Record results\n",
    "def describe_gpu():\n",
    "    loan_rdf.describe()\n",
    "\n",
    "def describe_cpu():\n",
    "    loan_pdf.describe()\n",
    "\n",
    "#display(loan_rdf.describe().to_pandas())\n",
    "\n",
    "run_times.add_result(\"describe\", \"gpu\", time_command(describe_gpu))\n",
    "run_times.add_result(\"describe\", \"cpu\", time_command(describe_cpu))\n",
    "\n",
    "run_times.display_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding (OHE) Performance Comparison\n",
    "\n",
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.\n",
    "\n",
    "Currently, one hot endcoding for Rapids requires the column that is to be encoded to be an integer or float, not a string.  You will need to create an integer column prior to using this!  You can use the hash_encode method to accomplish this, although you lose a little bit of readability.  In future versions of the software this is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:21.691651Z",
     "start_time": "2019-09-24T20:10:20.824262Z"
    }
   },
   "outputs": [],
   "source": [
    "# CPU / pandas example\n",
    "ohe_cpu_df = pd.get_dummies(loan_pdf['grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:22.297087Z",
     "start_time": "2019-09-24T20:10:21.693533Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU / Rapids example\n",
    "# cudf 0.9 cudf.reshape.general.get_dummies(df, prefix=None, prefix_sep='_', dummy_na=False, columns=None, cats={}, sparse=False, drop_first=False, dtype='int8')\n",
    "\n",
    "# Needed 50 hash values to get uniqueness ... probably a better way, but for now lets move on\n",
    "# print(ohe_gpu_df.grade.hash_encode(stop=50).value_counts())\n",
    "# print(ohe_gpu_df.grade.hash_encode(stop=50))\n",
    "# print(ohe_gpu_df.grade.hash_encode(stop=50))\n",
    "\n",
    "MAX_VAL=50\n",
    "loan_rdf['grade_hash'] = loan_rdf['grade'].hash_encode(stop=MAX_VAL)\n",
    "ohe_gpu_df = loan_rdf.one_hot_encoding(column='grade_hash', prefix='g', cats=[27,45,36,28,48,17,25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:22.415546Z",
     "start_time": "2019-09-24T20:10:22.299274Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare the results\n",
    "print(\"Pandas ...\")\n",
    "display(ohe_cpu_df.head(10))\n",
    "#pgdf(ohe_gpu_df[ohe_gpu_df['grade']=='A'].head(20))\n",
    "print(\"Rapids ...\")\n",
    "ohe_gpu_df = ohe_gpu_df.rename({\"g_27\": \"A\",\"g_45\": \"B\",\"g_36\": \"C\",\"g_28\": \"D\",\"g_48\": \"E\",\"g_17\": \"F\",\"g_25\": \"G\"})\n",
    "pgdf(ohe_gpu_df[['A','B','C','D','E','F','G']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:23.420014Z",
     "start_time": "2019-09-24T20:10:22.417543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Record the results\n",
    "\n",
    "def ohe_cpu() :\n",
    "    pd.get_dummies(loan_pdf['grade'])\n",
    "\n",
    "def ohe_gpu() :\n",
    "    MAX_VAL=50\n",
    "    loan_rdf['grade_hash'] = loan_rdf['grade'].hash_encode(stop=MAX_VAL)\n",
    "    ohe_gpu_df = loan_rdf.one_hot_encoding(column='grade_hash', prefix='g', cats=[27,45,36,28,48,17,25])\n",
    "\n",
    "run_times.add_result(\"one_hot_encode\", \"cpu\", time_command(ohe_cpu))\n",
    "run_times.add_result(\"one_hot_encode\", \"gpu\", time_command(ohe_gpu))\n",
    "run_times.display_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter with Date and Time ops - Performance comparison\n",
    "\n",
    "Current datetime functionality is limited to filtering data set for specific times.  Datetime doesn't not yet support math operations.\n",
    "\n",
    "Here we will find loan applicants that have a credit line prior to 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:23.425695Z",
     "start_time": "2019-09-24T20:10:23.421757Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "search_date = dt.datetime.strptime('2010-01-01', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:24.233993Z",
     "start_time": "2019-09-24T20:10:23.427409Z"
    }
   },
   "outputs": [],
   "source": [
    "# CPU / pandas\n",
    "query_cpu=loan_pdf.query('earliest_cr_line <= @search_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:25.947999Z",
     "start_time": "2019-09-24T20:10:24.235687Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU / Rapids\n",
    "query_gpu=loan_rdf.query('earliest_cr_line <= @search_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:26.033910Z",
     "start_time": "2019-09-24T20:10:25.951973Z"
    }
   },
   "outputs": [],
   "source": [
    "# compare results\n",
    "display(query_cpu.head())\n",
    "pgdf(query_gpu.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:29.225868Z",
     "start_time": "2019-09-24T20:10:26.037027Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter Record results\n",
    "def filter_cpu():\n",
    "    loan_pdf.query('earliest_cr_line <= @search_date')\n",
    "    \n",
    "def filter_gpu():\n",
    "    loan_rdf.query('earliest_cr_line <= @search_date')\n",
    "    \n",
    "run_times.add_result(\"filter_dt\", \"cpu\", time_command(filter_cpu,repeat=3))\n",
    "run_times.add_result(\"filter_dt\", \"gpu\", time_command(filter_gpu,repeat=3))\n",
    "run_times.display_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by value\n",
    "\n",
    "Sorting is a very expensive operation in data preparation so its useful to evaluate the performance of method.  Here we select a column to sort by and then compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:32.759646Z",
     "start_time": "2019-09-24T20:10:29.227582Z"
    }
   },
   "outputs": [],
   "source": [
    "# CPU / pandas\n",
    "sort_cpu=loan_pdf.sort_values(by='fico_range_high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:33.445996Z",
     "start_time": "2019-09-24T20:10:32.761463Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU / Rapids\n",
    "sort_gpu=loan_rdf.sort_values(by='fico_range_high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:33.519953Z",
     "start_time": "2019-09-24T20:10:33.447761Z"
    }
   },
   "outputs": [],
   "source": [
    "# compare results\n",
    "display(sort_cpu.head())\n",
    "pgdf(sort_gpu.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:41.372987Z",
     "start_time": "2019-09-24T20:10:33.521647Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sorting Record results\n",
    "def sort_cpu():\n",
    "    loan_pdf.sort_values(by='fico_range_high')\n",
    "    \n",
    "def sort_gpu():\n",
    "    loan_rdf.sort_values(by='fico_range_high')\n",
    "    \n",
    "run_times.add_result(\"sorting\", \"cpu\", time_command(sort_cpu, repeat=2))\n",
    "run_times.add_result(\"sorting\", \"gpu\", time_command(sort_gpu, repeat=2))\n",
    "run_times.display_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms and Custom functions\n",
    "\n",
    "Here we demonstrate how fast Rapids is at creating histogram bins.  We use the loan_amount column with a custom function to create a loan_bins column.  Then we grab the value counts using both Pandas and Rapids to get a rough comparison of the speed of these types of operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:41.379493Z",
     "start_time": "2019-09-24T20:10:41.375189Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom function example : creates simple bins for loan_amount histogram\n",
    "def roundto(num):\n",
    "    roundto=5000\n",
    "    a = int(num / roundto)\n",
    "    return float(a*roundto) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:51.980850Z",
     "start_time": "2019-09-24T20:10:41.381828Z"
    }
   },
   "outputs": [],
   "source": [
    "# CPU / pandas\n",
    "\n",
    "loan_pdf['loan_bins'] = loan_pdf.loan_amnt.apply(roundto)\n",
    "loan_pdf['loan_bins'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:10:52.813895Z",
     "start_time": "2019-09-24T20:10:51.982517Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU / rapids\n",
    "loan_rdf['loan_bins'] = loan_rdf.loan_amnt.applymap(roundto)\n",
    "print(loan_rdf['loan_bins'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:03.665902Z",
     "start_time": "2019-09-24T20:10:52.815944Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Record the results\n",
    "def hist_cpu() :\n",
    "    loan_pdf['loan_bins'] = loan_pdf.loan_amnt.apply(roundto)\n",
    "    loan_pdf['loan_bins'].value_counts()\n",
    "\n",
    "def hist_gpu() :\n",
    "    loan_rdf['loan_bins'] = loan_rdf.loan_amnt.applymap(roundto)\n",
    "    loan_rdf['loan_bins'].value_counts()\n",
    "\n",
    "run_times.add_result(\"histogram_ops\", \"cpu\", time_command(hist_cpu,repeat=1))\n",
    "run_times.add_result(\"histogram_ops\", \"gpu\", time_command(hist_gpu,repeat=1))\n",
    "run_times.display_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby \n",
    "\n",
    "Here we perform some aggregation on the lending club data set to get some **per grade statistics**.  For this exercise we will compare the speed of aggregating over Pandas dataframes and Rapids dataframes using the **groupby** function as shown in the [Rapids documentation](https://docs.rapids.ai/api/cudf/stable/) .  Notice how the syntax is exactly the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:05.189145Z",
     "start_time": "2019-09-24T20:11:03.667759Z"
    }
   },
   "outputs": [],
   "source": [
    "# CPU / Pandas\n",
    "# stats by grade\n",
    "grade_stats_pdf = loan_pdf.groupby('grade', as_index=False).agg({\"annual_inc\": [\"count\",\"mean\"], \"loan_amnt\": [\"count\",\"mean\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:05.819207Z",
     "start_time": "2019-09-24T20:11:05.190887Z"
    }
   },
   "outputs": [],
   "source": [
    "#GPU / Rapids\n",
    "# stats by grade\n",
    "grade_stats_rdf = loan_rdf.groupby('grade', as_index=False).agg({\"annual_inc\": [\"count\",\"mean\"], \"loan_amnt\": [\"count\",\"mean\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:05.846808Z",
     "start_time": "2019-09-24T20:11:05.820912Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grade summary statistics\n",
    "display(grade_stats_pdf)\n",
    "pgdf(grade_stats_rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:14.068611Z",
     "start_time": "2019-09-24T20:11:05.848411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Record the results\n",
    "\n",
    "def groupby_cpu() :\n",
    "    loan_pdf.groupby('grade', as_index=False).agg({\"annual_inc\": [\"count\",\"mean\"], \"loan_amnt\": [\"count\",\"mean\"]})\n",
    "\n",
    "def groupby_gpu() :\n",
    "    loan_rdf.groupby('grade', as_index=False).agg({\"annual_inc\": [\"count\",\"mean\"], \"loan_amnt\": [\"count\",\"mean\"]})\n",
    "\n",
    "run_times.add_result(\"groupby_ops\", \"cpu\", time_command(groupby_cpu,repeat=4))\n",
    "run_times.add_result(\"groupby_ops\", \"gpu\", time_command(groupby_gpu,repeat=4))\n",
    "run_times.display_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join \n",
    "\n",
    "Joining two dataframes can be an extremely computationally expensive task.  **Here we take the grade summary statistics computed in the groupby experiment above, and join it back with our table using grade as the key**.  This is a common practice in machine learning to apply average values per group back to the individual row.  This is a form of [mean encoding](https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:16.937899Z",
     "start_time": "2019-09-24T20:11:14.070361Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas Join\n",
    "loan_join_pdf = loan_pdf.set_index('grade').join(grade_stats_pdf.set_index('grade'),on=\"grade\",how=\"left\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:18.041307Z",
     "start_time": "2019-09-24T20:11:16.942142Z"
    }
   },
   "outputs": [],
   "source": [
    "#cuDF Join\n",
    "loan_rdf.set_index('grade').join(grade_stats_rdf.set_index('grade'),on=\"grade\",how=\"left\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:21.302128Z",
     "start_time": "2019-09-24T20:11:18.043643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Record the results\n",
    "def join_cpu() :\n",
    "    loan_pdf.set_index('grade').join(grade_stats_pdf.set_index('grade'),on=\"grade\",how=\"left\").reset_index()\n",
    "def join_gpu() :\n",
    "    loan_rdf.set_index('grade').join(grade_stats_rdf.set_index('grade'),on=\"grade\",how=\"left\").reset_index()\n",
    "\n",
    "run_times.add_result(\"join_ops\", \"cpu\", time_command(join_cpu,repeat=1))\n",
    "run_times.add_result(\"join_ops\", \"gpu\", time_command(join_gpu,repeat=1))\n",
    "run_times.display_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (cuML and sklearn) - Performance comparison\n",
    "\n",
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/techu-pca.png\"  width=\"200\" height=\"125\" align=\"middle\"/>\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables.\n",
    "\n",
    "A simple way to think about PCA is that it helps compress the data in a lossy representation of the original dataset.\n",
    "\n",
    "**Lets compare the performance of the Sklearn (cpu-based) implemenation vs cuML!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:21.307698Z",
     "start_time": "2019-09-24T20:11:21.303869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to normalize GPU dataframe function\n",
    "def normalize_df(gdf) :\n",
    "    for col in gdf.columns :\n",
    "        gdf[col] = (gdf[col] - gdf[col].mean()) / gdf[col].std()\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T13:50:47.173784Z",
     "start_time": "2019-09-11T13:50:47.171460Z"
    }
   },
   "source": [
    "### Prepare the data for PCA [not timed]\n",
    "Here we do some initial data preparation to normalize the dataframe columns.  We arent comparing performance of this step, its just to get us ready to do the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:22.265390Z",
     "start_time": "2019-09-24T20:11:21.309253Z"
    }
   },
   "outputs": [],
   "source": [
    "X_cols = list(loan_rdf.columns)\n",
    "print(\"Analysis Continuing with {}\".format(X_cols))\n",
    "X_cols.remove('default')\n",
    "X_cols.remove('grade')\n",
    "X_cols.remove('grade_hash')\n",
    "X_cols = [x for x in X_cols if loan_rdf[x].dtype == \"float64\" or loan_rdf[x].dtype == \"int8\"]\n",
    "print(\"Analysis Continuing with {}\".format(X_cols))\n",
    "# All types must be same ....\n",
    "for x in X_cols :\n",
    "    loan_rdf[x] = loan_rdf[x].astype(\"float64\")\n",
    "\n",
    "#print(loan_rdf[X_cols].dtypes)\n",
    "print(\"Normalizing dataframe prior to PCA\")\n",
    "loan_norm_rdf = normalize_df(loan_rdf[X_cols])\n",
    "print(\"Copying dataframe to pandas\")\n",
    "loan_norm_pdf = loan_norm_rdf.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:22.675726Z",
     "start_time": "2019-09-24T20:11:22.267102Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Normalized Dataframe\")\n",
    "print(loan_norm_rdf[X_cols].dtypes)\n",
    "pgdf(loan_norm_rdf) #.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) Performance \n",
    "\n",
    "We will compare the runtimes of PCA on CPU and then on GPU and also compare the results to make sure they are the same.  \n",
    "\n",
    "Here we take the normalized frame we built above and copy to pandas.  The two dataframes we will be working with are \n",
    "\n",
    "* loan_norm_pdf : normalized pandas dataframe\n",
    "* loan_norm_rdf : normalized GPU/RAPIDS dataframe\n",
    "\n",
    "these are exactly the same dataframe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:30.583538Z",
     "start_time": "2019-09-24T20:11:22.677616Z"
    }
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "# Both import methods supported\n",
    "from cuml import PCA\n",
    "from cuml.decomposition import PCA as PCA_gpu\n",
    "from sklearn.decomposition import PCA as PCA_cpu\n",
    "n_components=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:42.969911Z",
     "start_time": "2019-09-24T20:11:30.585170Z"
    }
   },
   "outputs": [],
   "source": [
    "# RUN PCA ! : CPU / Sklearn implementation\n",
    "pca_loan_cpu = PCA_cpu(n_components=n_components)\n",
    "pca_loan_cpu.fit(loan_norm_pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:44.608054Z",
     "start_time": "2019-09-24T20:11:42.971494Z"
    }
   },
   "outputs": [],
   "source": [
    "# RUN PCA ! : GPU / cuML implementation\n",
    "pca_loan_gpu = PCA_gpu(n_components=n_components)\n",
    "pca_loan_gpu.fit(loan_norm_rdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare results** : For PCA we use a scree plot to compare the results.  Scree plots show how much variance in the dataset is explained by each additional principal component.  Below, run the cell and just eyeball the graphs and convince yourself they are the same\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:11:44.963346Z",
     "start_time": "2019-09-24T20:11:44.610054Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare results ...\n",
    "\n",
    "display(pca_scree(pca_loan_cpu.explained_variance_ratio_, \"CPU\"))\n",
    "pca_scree(pca_loan_gpu.explained_variance_ratio_, \"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:12:10.777748Z",
     "start_time": "2019-09-24T20:11:44.964912Z"
    }
   },
   "outputs": [],
   "source": [
    "# record PCA performance results\n",
    "def pca_cpu() :    \n",
    "    print(\"cpu pca\")\n",
    "    pca_loan_cpu = PCA_cpu(n_components=n_components)\n",
    "    pca_loan_cpu.fit(loan_norm_pdf)\n",
    "\n",
    "\n",
    "def pca_gpu() :\n",
    "    pca_loan_gpu = PCA_gpu(n_components=n_components)\n",
    "    pca_loan_gpu.fit(loan_norm_rdf)\n",
    "\n",
    "    \n",
    "#print(loan_norm_rdf.shape)    \n",
    "run_times.add_result(\"pca\", \"gpu\", time_command(pca_gpu, repeat=2))\n",
    "run_times.add_result(\"pca\", \"cpu\", time_command(pca_cpu, repeat=2))\n",
    "\n",
    "run_times.display_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (cuML / sklearn // snapML)\n",
    "\n",
    "Linear regression is one of the most common algorithms applied to structured data.  Its useful when trying to make a prediction of a continuous variable.  For example, you could use linear regression to try and predict the total expected payment of a loan given historical data about default rates.  Lets try this below with our data set.  (Note lending club doesn't explicity provide this data in its data set, so we will use a fictitious total_payment column in our analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:12:14.468015Z",
     "start_time": "2019-09-24T20:12:10.779488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Regression : CPU / Sklearn\n",
    "from sklearn.linear_model import LinearRegression as LRSKL\n",
    "X = loan_norm_rdf.to_pandas()\n",
    "y = loan_rdf['default'].to_pandas()    \n",
    "lr_cpu = LRSKL(fit_intercept = True, normalize = False)\n",
    "res_cpu = lr_cpu.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:12:14.638019Z",
     "start_time": "2019-09-24T20:12:14.470499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Regression : GPU / Rapids cuML example\n",
    "from cuml.linear_model import LinearRegression as LRCUML\n",
    "X = loan_norm_rdf\n",
    "y2 = loan_rdf['default'].astype(\"float64\")    \n",
    "lr_gpu = LRCUML(fit_intercept = True, normalize = False) #, algorithm = \"eig\")\n",
    "res_gpu = lr_gpu.fit(X,y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:12:14.872926Z",
     "start_time": "2019-09-24T20:12:14.639681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"Coefficients:\")\n",
    "print(res_cpu.coef_)\n",
    "print(\"intercept:\")\n",
    "print(res_cpu.intercept_)\n",
    "\n",
    "print(\"Coefficients:\")\n",
    "print(res_gpu.coef_)\n",
    "print(\"intercept:\")\n",
    "print(res_gpu.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:12:33.456847Z",
     "start_time": "2019-09-24T20:12:14.874555Z"
    }
   },
   "outputs": [],
   "source": [
    "#Record Results \n",
    "\n",
    "# CPU \n",
    "def lr_cpu() :\n",
    "    lr_cpu = LRSKL(fit_intercept = True, normalize = False)\n",
    "    res = lr_cpu.fit(X,y)\n",
    "    \n",
    "X = loan_norm_rdf.to_pandas()\n",
    "y = loan_rdf['default'].to_pandas()    \n",
    "run_times.add_result(\"linear_reg\", \"cpu\", time_command(lr_cpu, repeat=5))\n",
    "\n",
    "\n",
    "# GPU\n",
    "def lr_gpu() :\n",
    "    lr_gpu = LRCUML(fit_intercept = True, normalize = False, algorithm = \"eig\")\n",
    "    res = lr_gpu.fit(X,y)\n",
    "\n",
    "X = loan_norm_rdf\n",
    "y = loan_rdf['default'].astype(\"float64\")    \n",
    "run_times.add_result(\"linear_reg\", \"gpu\", time_command(lr_gpu, repeat=5))\n",
    "\n",
    "\n",
    "\n",
    "run_times.display_results()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this lab we covered a number of common functions used by both data engineers and data scientists to manipulate dataframes and also build machine learning models.  The RAPIDS implementation demonstrates how much time you can save by running a lot of these operations on the GPU.   As data set sizes grow, and the number of experiments required increase, this performance gain can be a real advantage for getting to the answers faster.  Lets recap your speedups here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T15:48:21.459213Z",
     "start_time": "2019-09-24T15:48:21.453749Z"
    }
   },
   "outputs": [],
   "source": [
    "run_times.display_results()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T11:11:46.196739Z",
     "start_time": "2019-09-23T11:11:46.191946Z"
    }
   },
   "source": [
    "Note, you can play with the dataset size and rerun the notebook to see how that impacts your run results!  TL;DR the larger your dataframe the better the GPU speedups ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T10:33:01.500418Z",
     "start_time": "2019-09-23T10:33:01.498057Z"
    }
   },
   "source": [
    "## Credits\n",
    "\n",
    "This notebook was built by  Dustin VanStee (vanstee@us.ibm.com) from IBM Worldwide Client Experience Centers.  Special thanks to Steve LaFalce and Loic Fura for reviewing the content and suggesting edits.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
