{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Explore Some Telco Churn Data\n",
    "\n",
    "Data can be found here WA_Fn-UseC_-Telco-Customer-Churn.csv: https://www.kaggle.com/blastchar/telco-customer-churn/download\n",
    "\n",
    "# What’s in the Telco Customer Churn data set?\n",
    "This data set provides info to help you predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\n",
    "\n",
    "A telecommunications company is concerned about the number of customers leaving their landline business for cable competitors. They need to understand who is leaving. Imagine that you’re an analyst at this company and you have to find out who is leaving and why.\n",
    "\n",
    "The data set includes information about:\n",
    "* Customers who left within the last month – the column is called Churn\n",
    "* Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
    "* Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n",
    "* Demographic info about customers – gender, age range, and if they have partners and dependents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.453938Z",
     "start_time": "2020-05-07T19:29:19.960304Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load churnlab_utils.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from itertools import compress\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import tensorflow as tf\n",
    "\t\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap,aspect='auto')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('confusion_mat.png', bbox_inches='tight', format='png', dpi=300, pad_inches=0,transparent=True)\n",
    "    plt.show()\n",
    "    return\n",
    "\t\n",
    "def pdutil_describeItemRange(df):  # Show the number of unique values in each column\n",
    "    \"\"\"pdutil_describeItemRange performs is similar to descrive for a DataFrame\"\"\"\n",
    "    \"\"\"but it displays the number of unique values in each column\"\"\"\n",
    "    \"\"\"argument is a dataFrame \"\"\"\n",
    "    \"\"\"return is smaller dataFrame with the same columns as the original\"\"\"\n",
    "    cols = df.columns.tolist()\n",
    "    vals = pd.DataFrame ( [ len(set(df[s])) for s in df.columns.tolist()] ).T\n",
    "    vals.columns = cols\n",
    "    return vals\n",
    "\n",
    "def pdutil_factorize(df):\n",
    "    \"\"\"pdutil_factorize performs converts every categorical column in a dataFrame to numeric values\"\"\"\n",
    "    \"\"\"pass in a DataFrame\"\"\"\n",
    "    \"\"\"return value is a similar dataFrame but with categorical values converted to numbers\"\"\"    \n",
    "    return  df.apply(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "def pdutil_OneHotEncode(pdold, StaticCols, EncodeCols):\n",
    "    \"\"\"pdutil_OneHotEncode performs OneHotEncoding on specified columns in our dataframe\"\"\"\n",
    "    \"\"\"pdold is the old dataframe we are paasing in\"\"\"\n",
    "    \"\"\"StaticCols is a list of columns we do not wish to encode\"\"\"\n",
    "    \"\"\"EncodeCols is a list of columns we wish to encode\"\"\"\n",
    "    \"\"\"Returns a modified DataFrame with oneHotEncoded columns\"\"\"\n",
    "    # make sure that we our static list is not in our encode list\n",
    "    cols_OneHotEncode = sorted(list(set(EncodeCols) - set(StaticCols)))\n",
    "    # create a new dataframe which represents the static columns\n",
    "    pdnew = pdold[StaticCols]\n",
    "    # loop thru the endcode list and OneHotEncode (using get_dummies) each column in the encode list\n",
    "    # note - this will typically map a single column to several columns - one for each value contained in the original column\n",
    "    for col in cols_OneHotEncode:\n",
    "        new_names = []\n",
    "        enc = pd.get_dummies(pdold[col])\n",
    "        for subcol in  enc:\n",
    "            name = col + str(subcol)\n",
    "            new_names.append(name)\n",
    "        enc.columns = new_names\n",
    "        # concat the new OneHotEnoded dataframe to the original\n",
    "        pdnew = pd.concat([pdnew, enc], axis=1)\n",
    "    return pdnew\n",
    "\n",
    "\n",
    "def pdutil_dbscan(df, eps_):\n",
    "# Compute DBSCAN\n",
    "    labels = []\n",
    "    db = DBSCAN(eps=eps_, min_samples=100).fit(df)\n",
    "    labels = pd.DataFrame(db.labels_ )\n",
    "    #n_clusters_ = labels[0].madf()\n",
    "    n_clusters_ = len(list(set(db.labels_))) - (1 if -1 in db.labels_ else 0)\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    tdft = 'dbsPCA'\n",
    "    df[tdft]=labels\n",
    "    print('eps:{} Estimated number of clusters: {}'.format(eps_, n_clusters_) ) \n",
    "    return df, labels, n_clusters_\n",
    "\n",
    "## import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "CLASS_SIZE = 2\n",
    "#DATA_SIZE = 0\n",
    "\n",
    "def load_csv(filename):\n",
    "    file = pd.read_csv(filename, header=0)\n",
    "\n",
    "    # get sample's metadata\n",
    "    n_samples = int(file.columns[0])\n",
    "    n_features = int(file.columns[1])\n",
    "\n",
    "    # divide samples into explanation variables and target variable\n",
    "    data = np.empty((n_samples, n_features))\n",
    "    target = np.empty((n_samples,), dtype=np.int)\n",
    "    for i, row in enumerate(file.itertuples()):\n",
    "        target[i] = np.asarray(row[-1], dtype=np.int)\n",
    "        data[i] = np.asarray(row[1:n_features+1], dtype=np.float64)\n",
    "    return (data, target)\n",
    "\n",
    "# output train data \n",
    "def get_batch_data(x_train, y_train, size=None):\n",
    "    if size is None:\n",
    "        size = len(x_train)\n",
    "    batch_xs = x_train\n",
    "    batch_ys = []\n",
    "\n",
    "    # convert to 1-of-N vector\n",
    "    for i in range(len(y_train)):\n",
    "        val = np.zeros((CLASS_SIZE), dtype=np.float64)\n",
    "        val[y_train[i]] = 1.0\n",
    "        batch_ys.append(val)\n",
    "    batch_ys = np.asarray(batch_ys)\n",
    "    return batch_xs[:size], batch_ys[:size]\n",
    "\n",
    "# output test data\n",
    "def get_test_data(x_test, y_test):\n",
    "    batch_ys = []\n",
    "\n",
    "    # convert to 1-of-N vector\n",
    "    for i in range(len(y_test)):\n",
    "        val = np.zeros((CLASS_SIZE), dtype=np.float64)\n",
    "        val[y_test[i]] = 1.0\n",
    "        batch_ys.append(val)\n",
    "    return x_test, np.asarray(batch_ys)\n",
    "\n",
    "# for parameter initialize\n",
    "def get_stddev(in_dim, out_dim):\n",
    "    return 1.3 / math.sqrt(float(in_dim) + float(out_dim))\n",
    "\n",
    "# DNN Model Class\n",
    "class Classifier:\n",
    "    def __init__(self, hidden_units=[10], n_classes=0, data_size = 0):\n",
    "        self._hidden_units = hidden_units\n",
    "        self._n_classes = n_classes\n",
    "        self._data_size = data_size\n",
    "        self._sess = tf.Session()\n",
    "\n",
    "    # build model\n",
    "    def inference(self, x):\n",
    "        hidden = []\n",
    "\n",
    "        # Input Layer\n",
    "        with tf.name_scope(\"input\"):\n",
    "            weights = tf.Variable(tf.truncated_normal([self._data_size , self._hidden_units[0]], stddev=get_stddev(self._data_size, self._hidden_units[0]), seed=42), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([self._hidden_units[0]]), name='biases')\n",
    "            input = tf.matmul(x, weights) + biases\n",
    "\n",
    "        # Hidden Layers\n",
    "        for index, num_hidden in enumerate(self._hidden_units):\n",
    "            if index == len(self._hidden_units) - 1: break\n",
    "            with tf.name_scope(\"hidden{}\".format(index+1)):\n",
    "                weights = tf.Variable(tf.truncated_normal([num_hidden, self._hidden_units[index+1]], seed=42, stddev=get_stddev(num_hidden, self._hidden_units[index+1])), name='weights')\n",
    "                biases = tf.Variable(tf.zeros([self._hidden_units[index+1]]), name='biases')\n",
    "                inputs = input if index == 0 else hidden[index-1]\n",
    "                hidden.append(tf.nn.relu(tf.matmul(inputs, weights) + biases, name=\"hidden{}\".format(index+1)))\n",
    "        \n",
    "        # Output Layer\n",
    "        with tf.name_scope('output'):\n",
    "            weights = tf.Variable(tf.truncated_normal([self._hidden_units[-1], self._n_classes], seed=42, stddev=get_stddev(self._hidden_units[-1], self._n_classes)), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([self._n_classes]), name='biases')\n",
    "            logits = tf.nn.softmax(tf.matmul(hidden[-1], weights) + biases)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    # loss function\n",
    "    def loss(self, logits, y):        \n",
    "        #return -tf.reduce_mean(y * tf.log(logits))\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "    # fitting function for train data\n",
    "    def fit(self, x_train=None, y_train=None, steps=200):\n",
    "        # build model\n",
    "        x = tf.placeholder(tf.float32, [None, self._data_size ])\n",
    "        y = tf.placeholder(tf.float32, [None, CLASS_SIZE])\n",
    "        logits = self.inference(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        train_op = tf.train.AdamOptimizer(0.003).minimize(loss)\n",
    "\n",
    "        # save variables\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._logits = logits\n",
    " \n",
    "        # init parameters\n",
    "        #init = tf.initialize_all_variables() \n",
    "        init = tf.global_variables_initializer()\n",
    "        self._sess.run(init)\n",
    "\n",
    "        # train\n",
    "        for i in range(steps):\n",
    "            batch_xs, batch_ys = get_batch_data(x_train, y_train)\n",
    "            self._sess.run(train_op, feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "    # evaluation function for test data\n",
    "    def evaluate(self, x_test=None, y_test=None):\n",
    "        x_test, y_test = get_test_data(x_test, y_test)\n",
    "        \n",
    "        # build accuracy calculate step\n",
    "        correct_prediction = tf.equal(tf.argmax(self._logits, 1), tf.argmax(self._y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # evaluate\n",
    "        return self._sess.run([accuracy], feed_dict={self._x: x_test, self._y: y_test})\n",
    "\n",
    "    # label prediction\n",
    "    def predict(self, samples):\n",
    "        predictions = tf.argmax(self._logits, 1)\n",
    "        return self._sess.run(predictions, {self._x: samples})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose interactive or inline plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.466555Z",
     "start_time": "2020-05-07T19:29:20.458014Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Telco Customer Churn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.522770Z",
     "start_time": "2020-05-07T19:29:20.471222Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "# Added to support multiple environments\n",
    "#!https://www.kaggle.com/blastchar/telco-customer-churn/download\n",
    "churn = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.532222Z",
     "start_time": "2020-05-07T19:29:20.527806Z"
    }
   },
   "source": [
    "### Exercise - In the cell below fix the code to display first few rows of the churn dataset\n",
    "\n",
    " <div class=\"panel-group\" id=\"accordion-21\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-21\" href=\"#collapse1-21\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-21\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:\n",
    "          <br> churn.head(6)<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.747173Z",
     "start_time": "2020-05-07T19:29:20.536669Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 70)\n",
    "churn\n",
    "\n",
    "# Notice that the data in various columns might be strings, integers, floats, whatever\n",
    "# We need a way to convert all these items into numbers, this is part of data prep and data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big is my data, what shape is it?\n",
    "\n",
    "determine how many rows and columns of data you have using the shape attribute of your dataframe\n",
    "\n",
    " <div class=\"panel-group\" id=\"accordion-22\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-22\" href=\"#collapse1-22\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-22\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:\n",
    "          <br> churn.shape<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T21:18:53.241271Z",
     "start_time": "2020-05-06T21:18:53.236252Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much data prep do I have to do?\n",
    "\n",
    "Lets see which columns are already numbers versus some other data type, such as string or objects\n",
    "\n",
    "Here we see there are 18 object columsn that are NOT number types\n",
    "\n",
    "modify the code below so that the colors list is active (uncomment the line) so the code will correctly run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.776298Z",
     "start_time": "2020-05-07T19:29:20.752475Z"
    }
   },
   "outputs": [],
   "source": [
    "del colors  # cell is broken, fix it\n",
    "plt.ylim(0,18, 1)\n",
    "# colors = ['r', 'g', 'b', 'k', 'y', 'm', 'c'] \n",
    "\n",
    "# Create object type dataframe for plotting\n",
    "column_obj_df = pd.DataFrame(churn.dtypes,columns=['objectType'])\n",
    "display(column_obj_df)\n",
    "#Get a histogram the frequency of different data types in this dataframe\n",
    "coltype_histogram = column_obj_df.objectType.value_counts()\n",
    "print(\"Histogram = \".format(coltype_histogram))\n",
    "\n",
    "# Plot it!  Theres an error here, but look at the error message and fixup the code above.\n",
    "coltype_histogram.plot(kind='bar', color = colors )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Cleaning Required       :-)\n",
    "In reality, on **real datasets**, this takes on the order of **80%** of datascientist time.\n",
    "\n",
    "For this dataset, TotalCharges is an object rather than a float64. We will convert it to float64 and **force any non numeric items** to be imputed with the **mean**.\n",
    "\n",
    "Other methods might interpolate a missing value using a linear interpolation, or use the max value, or in some cases drop these instances or dro the feature - it all depends on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifically force dirty the data to make a point\n",
    "\n",
    "We will inject None, NaN, Inf into various columns to demonstrate what non-cleaned data might look like. \n",
    "\n",
    "This will motivate our examples of detecting and cleaning this kind of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.777556Z",
     "start_time": "2020-05-07T19:29:19.979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data was too clean - lets artifically inject some NULL data to see how to detect the,m in later steps\n",
    "\n",
    "nullmask1 = [5, 47, 1962, 1987, 1991, 1994, 2018]\n",
    "nullmask2 = [2, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31]\n",
    "nullmask3 = [3,13,23]\n",
    "churn.loc[nullmask1, 'SeniorCitizen'] = None\n",
    "churn.loc[nullmask2, 'TotalCharges'] = float('NaN')\n",
    "churn.loc[nullmask3, 'MonthlyCharges'] = float('Inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine how many NaN's, None's, Inf's exist in the data\n",
    "\n",
    "This can be done for the entire data set or on a column by column basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.779197Z",
     "start_time": "2020-05-07T19:29:19.981Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.mode.use_inf_as_na = True\n",
    "#churn.apply(pd.Series.nunique)   # cardinality\n",
    "print(churn.isnull().sum().sum())  # count Nan's in entire dataframe|\n",
    "#print(churn.isnull().sum() )  # count Nan's by column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Where the NaN's are: Bar Chart of Frequencies\n",
    "\n",
    "Here we use a variation of a histogram, implenmented as a bar plot, to count the frequencies of NaN's in our data set for each column.\n",
    "\n",
    "Nan's must be delat with prior to any modelling\n",
    "\n",
    "This is helpful to visualize - because it is a visual depiction of what is blocking you from creating models.\n",
    "Its a helpful chart to bring to other teams when asking for help \n",
    "\n",
    "**\"I can use your SME expertise here, These items are blocking me. In your opinion how should I deal with NaNs and INF, and Nones?  Should I drop the row? replace with the mean or max or min or some other value?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.780826Z",
     "start_time": "2020-05-07T19:29:19.983Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "colors = ['r', 'g', 'b', 'k', 'y', 'm', 'c'] \n",
    "plt.figure(figsize=(15, 5))\n",
    "churn.isnull().sum().plot(kind='bar',color = colors, title='Number of NaNs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct Nan's or Reload the dataset \n",
    "\n",
    "Reloading the data to undo the Nan experiment from above.\n",
    "\n",
    "These are ways you can implement an SME's suggestion to fill NANs or INF or whatever with values\n",
    "\n",
    "#### ??? does the code below error out ?\n",
    "\n",
    "We are trying to fillna with a value - why does it crash - spend a few minutes troubleshooting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big is my data, what shape is it?\n",
    "\n",
    "determine how many rows and columns of data you have using the shape attribute of your dataframe\n",
    "\n",
    " <div class=\"panel-group\" id=\"accordion-23\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-23\" href=\"#collapse1-23\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-23\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">In a sandbox cell run this:\n",
    "          <br> churn[\"TotalCharges\"].mean() <br>\n",
    "          <br> and then run this <br>\n",
    "          <br> churn[\"TotalCharges\"].mean <br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.782225Z",
     "start_time": "2020-05-07T19:29:19.985Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal : Fill the TotalCharges cells that have NaN with the mean value of the columns\n",
    "#harder problem - see solutioin in stack overflow or below\n",
    "churn[\"TotalCharges\"].fillna(churn[\"TotalCharges\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.784144Z",
     "start_time": "2020-05-07T19:29:19.986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here's a code fix so all Nan's and inf can be removed\n",
    "\n",
    "# the nans and inf's from before are apparently treated as strings in an othersie numeric column and caused some errors\n",
    "# stack overflow is your friend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.785820Z",
     "start_time": "2020-05-07T19:29:19.988Z"
    }
   },
   "outputs": [],
   "source": [
    "churn[['TotalCharges']] = churn[['TotalCharges']].apply(pd.to_numeric, errors='coerce') \n",
    "churn[\"TotalCharges\"].fillna(churn[\"TotalCharges\"].mean(), inplace=True)\n",
    "\n",
    "churn[['SeniorCitizen']] = churn[['SeniorCitizen']].apply(pd.to_numeric, errors='coerce') \n",
    "churn[\"SeniorCitizen\"].fillna(churn[\"SeniorCitizen\"].mean(), inplace=True)\n",
    "\n",
    "churn.MonthlyCharges.replace(np.inf, 0, inplace=True)\n",
    "\n",
    "churn[['MonthlyCharges']] = churn[['MonthlyCharges']].apply(pd.to_numeric, errors='coerce') \n",
    "churn[\"MonthlyCharges\"].fillna(churn[\"MonthlyCharges\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION\n",
    "\n",
    "Describe the Range of Values in each Column\n",
    "The is an important step in understanding your data. Not only should I have an understanding of the size and shape of my data, the number of rows and columns, but also how many unique values each column may take on. This is important for upcoming steps where we encode the data for consumption by a model. knowing how many possible values can influence the choice of encodig such one-hot-encoding versus binning for example. For classification purposes, the categorical values (most of the entirety of this table) have 2 to 4 possible values and might be candidates for one-hot-encoding or similar encodings.  But MonthlyCharges and TotalCharges, if manipulated at all, might benefit from binning, since this is more of a continous variable and not categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.787487Z",
     "start_time": "2020-05-07T19:29:19.990Z"
    }
   },
   "outputs": [],
   "source": [
    "pdutil_describeItemRange(churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.789299Z",
     "start_time": "2020-05-07T19:29:19.991Z"
    }
   },
   "outputs": [],
   "source": [
    "churn.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Cardinality of the Dataset Visually\n",
    "\n",
    "Here we explore the cardinality of the data - how many varieties of values are ther in each column.\n",
    "\n",
    "Categorical data hopefully will have a handful of values, numeric data will likely have many attainable values.\n",
    "\n",
    "This step is for sanity checking. Do expect a smaller number of choices for a given column to be able to acquire?\n",
    "\n",
    "What if the categorical data has huge cardinality? For example, zip codes have huge cardinality. Some researchers recommend \n",
    "\n",
    "replacing zip code with a numerica value that represents a presentage - such as perentage of each zip codes contribution to sales, or percentage of of occurences of that zip code out of all zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.790881Z",
     "start_time": "2020-05-07T19:29:19.993Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "churn.apply(pd.Series.nunique).plot(kind='bar', color = ['r', 'g', 'b', 'k', 'y', 'm', 'c'], title='Number of Unique Valuess', ylim=(0,10) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T21:23:03.258814Z",
     "start_time": "2020-05-06T21:23:03.247837Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T21:17:59.609639Z",
     "start_time": "2020-05-06T21:17:59.522Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break Dataset into Two Parts: Categorical, and Numeric\n",
    "\n",
    "Breaking apart just to handle each group conceptually differently. More work required on categrical data\n",
    "\n",
    "churn_cat_col will be a list indicating which are categorical\n",
    "churn_num_col will be a list indicating which are numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.792969Z",
     "start_time": "2020-05-07T19:29:19.998Z"
    }
   },
   "outputs": [],
   "source": [
    "# here are the columns which are categorical\n",
    "churn_cat_col = [key for key in dict(churn.dtypes) if dict(churn.dtypes)[key] in ['object']]\n",
    "# here are the columns which are numeric\n",
    "churn_num_col = [key for key in dict(churn.dtypes) if dict(churn.dtypes)[key] in ['float64', 'int64']]\n",
    "#create a dataframe to hold categorical data\n",
    "churn_cat = churn[churn_cat_col]\n",
    "#create a dataframe to hold numeric data\n",
    "churn_num = churn[churn_num_col]\n",
    "# display first few rows of numerical data to get a feel for them\n",
    "print (churn_num.head())\n",
    "# display first few rows of categorical data to get a feel for them\n",
    "churn_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.794861Z",
     "start_time": "2020-05-07T19:29:20.001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute stats on numerical data - see if you have any aha moments\n",
    "churn_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T23:03:51.328433Z",
     "start_time": "2020-05-03T23:03:51.310464Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Categories to Numbers\n",
    "\n",
    "We apply the factorize function to all the categories as a first pass. We created a new dataframe here called **churn_cat_encode**\n",
    "\n",
    "The commented out code would allow us to map back to the original form if needed\n",
    "\n",
    "PaymentMethod for example will now have numeric values like 0, 1, 2, 3,...:\n",
    "\n",
    "Look at PaymentMethod column - did you notice that the cardinality is something like 4?\n",
    "\n",
    "SHould it matter when predicting from a model that PaymentMethod = 3 is a larger value than PaymentMethod 1? Or is the numerical ordering more accidental and not something a model should pay attention to?\n",
    "\n",
    "### Is this One hot encoding yet???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.796568Z",
     "start_time": "2020-05-07T19:29:20.005Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  pd_factorize the categorical churn columns\n",
    "# after this step all categories are nubers\n",
    "churn_cat_encode = pdutil_factorize(churn_cat)\n",
    "churn_cat_encode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.798531Z",
     "start_time": "2020-05-07T19:29:20.007Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a moment to find a pandas function to find unique value for columns\n",
    "# everything is numbers now - but do the numbers makes sense?\n",
    "# a column with 4 values 0,1,2,3  seems to indicates that 3 is stronger or more valueable or more heavily weights than any other tiem in the column\n",
    "# should this e the case for a given column?\n",
    "churn_cat_encode.nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the data (OneHotEncode)\n",
    "The problem with the data above is that while the data are numeric, it give arbitrary low weights to categories encoded with 0 and higher weights to others\n",
    "\n",
    "This will probably skew our analysis. For example, PaymentMethod has values 0, 1, 2.  Do we really mean to imply that PaymentMethod=0 has no weight, but PaymentMethod=2 has twice as much weight as PaymentMethod=1?\n",
    "\n",
    "We use OneHotEncode here to make sure each value that was used is encoded with the value 1, while the value will be 0 if that feature was not used.\n",
    "\n",
    "**Note:** We dont want to encode categories with many unique values in it such as customerID. \n",
    "\n",
    "We define a funvtion to handle doing the encoding for us\n",
    "\n",
    "Now PaymentMethod will be split into brand new columns (one for each kind of numeric value) and each cell will have either 0 or 1, kind of like a check box, indicating the feature was present or not.\n",
    "\n",
    "We also concatenate the previous numerical columns back in to have them available for analysis after standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.800604Z",
     "start_time": "2020-05-07T19:29:20.009Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of column name I wish to OneHotEncode\n",
    "cols_OneHotEncode = ['DeviceProtection','TechSupport','OnlineBackup','MultipleLines','gender','StreamingTV','Contract','StreamingMovies','PhoneService','PaperlessBilling','OnlineSecurity','Partner','Dependents','InternetService','PaymentMethod']\n",
    "# label is the column I ultimately want to predict aka Churn\n",
    "label = churn_cat_encode['Churn']\n",
    "# customer is the column representing the customerId\n",
    "customer =  churn_cat_encode['customerID']  # going to use later\n",
    "churn_encoded_numeric = pdutil_OneHotEncode (churn_cat_encode, [], cols_OneHotEncode)\n",
    "# join the two kinds of data - categoricaal (oneHotEncoded) to the original Numerical data\n",
    "churn_encoded_numeric = pd.concat([churn_encoded_numeric, churn_num], axis=1)  # add numeric to main df\n",
    "churn_encoded_numeric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.802220Z",
     "start_time": "2020-05-07T19:29:20.010Z"
    }
   },
   "outputs": [],
   "source": [
    "churn_encoded_numeric.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data: Plotting One Hot Encoded Data is Problematic\n",
    "\n",
    "You tend to get a few huge piles of dots that dont contribute much to understanding the data\n",
    "\n",
    "### Plotting in 3D Won't Help Here Either \n",
    "\n",
    "Part of what you will learn in this lab are ways to explore data that is hesitant to be explored\n",
    "\n",
    "How can we get a feel for our customers in the data frame if this is all I have to work with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.803792Z",
     "start_time": "2020-05-07T19:29:20.012Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = churn_encoded_numeric['DeviceProtection0']\n",
    "y = churn_encoded_numeric['InternetService0']\n",
    "z = churn_encoded_numeric['Contract2']\n",
    "\n",
    "ax.scatter(x, y, z, c='r', marker='o')\n",
    "\n",
    "ax.set_xlabel('DeviceProtection0')\n",
    "ax.set_ylabel('InternetService0')\n",
    "ax.set_zlabel('Contract2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.805347Z",
     "start_time": "2020-05-07T19:29:20.014Z"
    }
   },
   "outputs": [],
   "source": [
    "# even a scatter_matrix is NOT useful\n",
    "%matplotlib inline\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols_ = ['DeviceProtection0','InternetService0','Contract2','gender0','MultipleLines2']\n",
    "\n",
    "scatter_matrix(churn_encoded_numeric[cols_], alpha=0.4, figsize=[10,10], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize the Data\n",
    "\n",
    "We scale and center the data around the mean. The reason is to make sure that each column gets a fair vote in the upcoming pca.  Without scaling, a column such as total_charges, with large range of values with larger absolute magnitude could dominate in PCA and effectively become the defacto largest pca component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.807265Z",
     "start_time": "2020-05-07T19:29:20.015Z"
    }
   },
   "outputs": [],
   "source": [
    "# if you get this error:\n",
    "# ValueError: Input contains infinity or a value too large for dtype('float64')\n",
    "# then you did not clean up your nans\n",
    "from sklearn import preprocessing\n",
    "#scale our data to zero center it\n",
    "scaler = preprocessing.StandardScaler().fit(churn_encoded_numeric)\n",
    "X_scaled = preprocessing.scale(churn_encoded_numeric, with_mean=True, with_std=True )   # same as (df-df.mean())/df.std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data: Categorical \n",
    "\n",
    "## Copy Churn Column to Encoded Dataframe\n",
    "This dataframe has the same number of rows as the principal components Dataframe\n",
    "\n",
    "They are parallel with each other\n",
    "\n",
    "This copy step may be redundant since we copied Churn to the PCA Dataframe too, but it makes plotting more convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.809114Z",
     "start_time": "2020-05-07T19:29:20.017Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'Churn' not in churn_encoded_numeric.columns:\n",
    "    churn_encoded_numeric = pd.concat([churn_encoded_numeric, label], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.810748Z",
     "start_time": "2020-05-07T19:29:20.019Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols_ = ['DeviceProtection0','InternetService0','Contract2','gender0','MultipleLines2']\n",
    "\n",
    "scatter_matrix(churn_encoded_numeric[cols_], alpha=0.4, figsize=[10,10], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T21:31:56.762188Z",
     "start_time": "2020-05-03T21:31:56.751204Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data:  Correlation Plot\n",
    "Plot shows which factors are correlated with which ohers and to what degree.\n",
    "\n",
    "Here, red means highly correlated (as in correlated with Churn being bad therefor red), blue means negatively correlated, and white means no correlation.\n",
    "**Notice** A customer with more contracts, more dependents, more tenure are less likley to churn in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.812669Z",
     "start_time": "2020-05-07T19:29:20.021Z"
    }
   },
   "outputs": [],
   "source": [
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = churn_encoded_numeric.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.7, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.814493Z",
     "start_time": "2020-05-07T19:29:20.023Z"
    }
   },
   "outputs": [],
   "source": [
    "churn_encoded_numeric[(churn_encoded_numeric['Contract0'] == 1)].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.817092Z",
     "start_time": "2020-05-07T19:29:20.024Z"
    }
   },
   "outputs": [],
   "source": [
    "churn_encoded_numeric[(churn_encoded_numeric['Contract2'] == 1)].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:20.818838Z",
     "start_time": "2020-05-07T19:29:20.026Z"
    }
   },
   "outputs": [],
   "source": [
    "# from the above two cells, determine which customer group should you reach out to to keep them from churning at all costs? \n",
    "# maybe offer special promotions\n",
    "# could we crate a model to determine which of these loyal clients is likley to churn?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
