# AUTOGENERATED! DO NOT EDIT! File to edit: Rapids/01-Rapids-LendingClub-Benchmark-solution-power8.ipynb (unless otherwise specified).

__all__ = ['pgdf', 'time_command', 'COMPARE', 'run_times', 'filename', 'DATA_DOUBLE_FACTOR', 'loan_pdf', 'loan_rdf',
           'describe_gpu', 'describe_cpu', 'ohe_cpu', 'ohe_gpu', 'search_date', 'filter_cpu', 'filter_gpu', 'sort_cpu',
           'sort_gpu', 'roundto', 'hist_cpu', 'hist_gpu', 'grade_stats_pdf', 'grade_stats_rdf', 'groupby_cpu',
           'groupby_gpu', 'join_cpu', 'join_gpu', 'normalize_df', 'X_cols', 'X_cols', 'loan_norm_rdf', 'loan_norm_pdf',
           'n_components', 'pca_cpu', 'pca_gpu', 'lr_cpu', 'lr_gpu', 'X', 'y', 'X', 'y', 'pgdf', 'time_command',
           'COMPARE', 'run_times', 'filename', 'DATA_DOUBLE_FACTOR', 'loan_pdf', 'loan_rdf', 'describe_gpu',
           'describe_cpu', 'ohe_cpu', 'ohe_gpu', 'search_date', 'filter_cpu', 'filter_gpu', 'sort_cpu', 'sort_gpu',
           'roundto', 'hist_cpu', 'hist_gpu', 'grade_stats_pdf', 'grade_stats_rdf', 'groupby_cpu', 'groupby_gpu',
           'join_cpu', 'join_gpu', 'normalize_df', 'X_cols', 'X_cols', 'loan_norm_rdf', 'loan_norm_pdf', 'n_components',
           'pca_cpu', 'pca_gpu', 'lr_cpu', 'lr_gpu', 'X', 'y', 'X', 'y']

# Cell
# Setup in notebook flag
try: from nbdev.imports import IN_NOTEBOOK
except: IN_NOTEBOOK=False


# Cell
# Imports
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')
import time
import timeit

from datetime import datetime
import math

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

import glob
import os
import sys
sys.path.append('../utils/')

#dask
import dask
from dask import dataframe as dd

# Rapids
import cudf
from cudf import DataFrame as RapidsDataFrame
# cudf                      0.11.0          cuda10.2_py37_673.g45906b8    https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda-early-access
# libcudf                   0.11.0          cuda10.2_657.g7f5e265    https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda-early-access
import torch


# Cell
# [print gpu dataframe] helper function to print GPU dataframes
def pgdf(gdf) :
    display(gdf.to_pandas())

# Cell
def time_command(cmd,repeat=1) :
    avg_runtime = timeit.timeit(cmd, number=repeat)
    return float(avg_runtime / repeat)

# Cell
# Dictionary to store results ..
# example "describe" : {"gpu" : []}
# TODO : make display results look better ..
class COMPARE() :
        ## Abstract Custom Implementations
    def __init__(self) :
        #nprint("Loading Data.  Overriding __init__ from dfutils")
        self.tests = []
        self.gpu_results = {}
        self.cpu_results = {}
        self.df_shape = (0,0)
        self.df_memory_gb = 0

    def add_result(self, test_name, gpu_result, runtime) :
        if test_name not in self.tests :
            self.tests.append(test_name)
            self.gpu_results[test_name] = []
            self.cpu_results[test_name] = []

        if(gpu_result == "gpu") :
            self.gpu_results[test_name].append(runtime)
        else :
            self.cpu_results[test_name].append(runtime)

    def display_results(self) :
        print("Dataframe size : {} {} GB".format(self.df_shape, self.df_memory_gb))
        print("{:<20} {:<20} {:<20} {:<20}".format("test", "CPU(s)", "GPU(s)", "GPU Speedup"))
        for i in self.tests :
            cpu_mean = sum(self.cpu_results[i]) / (len(self.cpu_results[i])+0.00001)
            gpu_mean = sum(self.gpu_results[i]) / (len(self.gpu_results[i])+0.00001)
            su = cpu_mean / (gpu_mean + .00001)
            print("{:<20} {:<20.4f} {:<20.4f} {:<20.2f}".format(i, cpu_mean, gpu_mean, su ))

run_times = COMPARE()


# Cell

filename = None
if not IN_NOTEBOOK:
    filename = "../Rapids/loan_project_df.parquet.gzip"
else :
    # import data
    filename = "./loan_project_df.parquet.gzip"

# Expand data to highlight performance difference
# 3 ~ 1GB dataset
# 4 ~ 2GB dataset
# ... etc
DATA_DOUBLE_FACTOR=3

# Pandas dataframe
loan_pdf = pd.read_parquet(filename)#  , names=ts_cols,dtype=ts_dtypes,skiprows=1)

# Rapids Dataframe
loan_rdf = cudf.read_parquet(filename)#  , names=ts_cols,dtype=ts_dtypes,skiprows=1)

# Cell
# Scale up data to 10 million rows
for i in range(DATA_DOUBLE_FACTOR) :
    loan_pdf = pd.concat([loan_pdf,loan_pdf],axis=0)
    loan_rdf = cudf.concat([loan_rdf,loan_rdf],axis=0)
    loan_rdf = loan_rdf.reset_index().drop("index",axis=1)
    loan_pdf = loan_pdf.reset_index().drop("index",axis=1)
    #pgdf(loan_rdf.head())
    #display(loan_pdf.head())


# Cell

# Dataframe attributes
print("Rapids")
print("\nDataframe datatypes\n---------------------")
print(loan_rdf.dtypes)
print("\nDataframe Shape (rows,cols)\n---------------------")
print(loan_rdf.shape)
print("\nDataframe dimensions\n---------------------")
print(loan_rdf.ndim)
print("\nDataframe Column names\n---------------------")
print(loan_rdf.columns)


# Cell

# Dataframe attributes
print("\n\nPandas")
print("\nDataframe datatypes\n---------------------")
print(loan_pdf.dtypes)
print("\nDataframe Shape (rows,cols)\n---------------------")
print(loan_pdf.shape)
print("\nDataframe dimensions\n---------------------")
print(loan_pdf.ndim)
print("\nDataframe Column names\n---------------------")
print(loan_pdf.columns)
print("\nDataframe Memory Usage\n---------------------")
print(loan_pdf.memory_usage(index=True).sum())


# Cell

run_times.df_shape = loan_pdf.shape
run_times.df_memory_gb = loan_pdf.memory_usage(index=True).sum() /10**9

print("Initial Data set size ~= {:0.3f} GB for this experiment".format(run_times.df_memory_gb))


# Cell

# Record results
def describe_gpu():
    loan_rdf.describe()

def describe_cpu():
    loan_pdf.describe()

#display(loan_rdf.describe().to_pandas())

run_times.add_result("describe", "gpu", time_command(describe_gpu))
run_times.add_result("describe", "cpu", time_command(describe_cpu))

run_times.display_results()

# Cell
# Record the results

def ohe_cpu() :
    x=pd.get_dummies(loan_pdf['grade'])

def ohe_gpu() :
    tmpdf = cudf.DataFrame()
    tmpdf['grade'] = loan_pdf['grade']
    x=ohe_gpu_df = cudf.get_dummies(tmpdf)

run_times.add_result("one_hot_encode", "cpu", time_command(ohe_cpu))
run_times.add_result("one_hot_encode", "gpu", time_command(ohe_gpu))
run_times.display_results()


# Cell
import datetime as dt

search_date = dt.datetime.strptime('2010-01-01', '%Y-%m-%d')

# Cell
# Filter Record results
def filter_cpu():
    loan_pdf.query('earliest_cr_line <= @search_date')

def filter_gpu():
    loan_rdf.query('earliest_cr_line <= @search_date')

run_times.add_result("filter_dt", "cpu", time_command(filter_cpu,repeat=3))
run_times.add_result("filter_dt", "gpu", time_command(filter_gpu,repeat=3))
run_times.display_results()


# Cell
# Sorting Record results
def sort_cpu():
    loan_pdf.sort_values(by=['fico_range_high','loan_amnt'],ascending=True)

def sort_gpu():
    loan_rdf.sort_values(by=['fico_range_high','loan_amnt'],ascending=True)

run_times.add_result("sorting", "cpu", time_command(sort_cpu, repeat=2))
run_times.add_result("sorting", "gpu", time_command(sort_gpu, repeat=2))
run_times.display_results()


# Cell
# custom function example : creates simple bins for loan_amount histogram
def roundto(num):
    roundto=5000
    a = int(num / roundto)
    return float(a*roundto)


# Cell
# Record the results
def hist_cpu() :
    loan_pdf['loan_bins'] = loan_pdf.loan_amnt.apply(roundto)
    loan_pdf['loan_bins'].value_counts()

def hist_gpu() :
    loan_rdf['loan_bins'] = loan_rdf.loan_amnt.applymap(roundto)
    loan_rdf['loan_bins'].value_counts()

run_times.add_result("histogram_ops", "cpu", time_command(hist_cpu,repeat=3))
run_times.add_result("histogram_ops", "gpu", time_command(hist_gpu,repeat=3))
run_times.display_results()


# Cell
# CPU / Pandas
# stats by grade
#grade_stats_pdf = loan_pdf.groupby('grade', as_index=False).agg({"annual_inc": ["count","mean"], "loan_amnt": ["count","mean"]})
grade_stats_pdf = loan_pdf.groupby('grade', as_index=False).agg({"annual_inc": "mean", "loan_amnt": "mean"})


# Cell
#GPU / Rapids
# stats by grade
# grade_stats_rdf = loan_rdf.groupby('grade', as_index=False).agg({"annual_inc": "count","annual_inc": "mean", "loan_amnt": "count","loan_amnt":"mean"})
grade_stats_rdf = loan_rdf.groupby('grade', as_index=False).agg({"annual_inc": "mean","loan_amnt":"mean"})


# Cell
# Record the results

def groupby_cpu() :
    loan_pdf.groupby('grade', as_index=False).agg({"annual_inc": "mean", "loan_amnt": "mean"})
# export

def groupby_gpu() :
    loan_rdf.groupby('grade', as_index=False).agg({"annual_inc": "mean","loan_amnt":"mean"})

run_times.add_result("groupby_ops", "cpu", time_command(groupby_cpu,repeat=4))
run_times.add_result("groupby_ops", "gpu", time_command(groupby_gpu,repeat=4))
run_times.display_results()

# Cell
# Record the results
def join_cpu() :
    loan_join_pdf = loan_pdf.set_index('grade').join(grade_stats_pdf.set_index('grade'),lsuffix='_l',rsuffix='_r',on="grade",how="left").reset_index()
def join_gpu() :
    loan_join_rdf = loan_rdf.set_index('grade').join(grade_stats_rdf.set_index('grade'),lsuffix='_l',rsuffix='_r',on="grade",how="left").reset_index()

run_times.add_result("join_ops", "cpu", time_command(join_cpu,repeat=1))
run_times.add_result("join_ops", "gpu", time_command(join_gpu,repeat=1))
run_times.display_results()

# Cell
# Helper function to normalize GPU dataframe function
def normalize_df(gdf) :
    for col in gdf.columns :
        gdf[col] = (gdf[col] - gdf[col].mean()) / gdf[col].std()
    return gdf

# Cell
X_cols = list(loan_rdf.columns)
print("Analysis Continuing with {}".format(X_cols))
X_cols.remove('default')
X_cols.remove('grade')
# X_cols.remove('grade_hash')
X_cols = [x for x in X_cols if loan_rdf[x].dtype == "float64" or loan_rdf[x].dtype == "int8"]
print("Analysis Continuing with {}".format(X_cols))
# All types must be same ....
for x in X_cols :
    loan_rdf[x] = loan_rdf[x].astype("float64")

#print(loan_rdf[X_cols].dtypes)
print("Normalizing dataframe prior to PCA")
loan_norm_rdf = normalize_df(loan_rdf[X_cols])
print("Copying dataframe to pandas")
loan_norm_pdf = loan_norm_rdf.to_pandas()


# Cell
# PCA
# Both import methods supported
from cuml import PCA
from cuml.decomposition import PCA as PCA_gpu
from sklearn.decomposition import PCA as PCA_cpu
n_components=5


# Cell
# record PCA performance results
def pca_cpu() :
    print("cpu pca")
    pca_loan_cpu = PCA_cpu(n_components=n_components)
    pca_loan_cpu.fit(loan_norm_pdf)


def pca_gpu() :
    print("gpu pca")
    pca_loan_gpu = PCA_gpu(n_components=n_components)
    pca_loan_gpu.fit(loan_norm_rdf)


#print(loan_norm_rdf.shape)
run_times.add_result("pca", "gpu", time_command(pca_gpu, repeat=2))
run_times.add_result("pca", "cpu", time_command(pca_cpu, repeat=2))

run_times.display_results()

# Cell
from sklearn.linear_model import LinearRegression as LRSKL
from cuml.linear_model import LinearRegression as LRCUML


# Cell
#Record Results

# CPU
def lr_cpu() :
    lr_cpu = LRSKL(fit_intercept = True, normalize = False)
    res = lr_cpu.fit(X,y)

X = loan_norm_rdf.to_pandas()
y = loan_rdf['default'].to_pandas()
run_times.add_result("linear_reg", "cpu", time_command(lr_cpu, repeat=5))


# GPU
def lr_gpu() :
    lr_gpu = LRCUML(fit_intercept = True, normalize = False, algorithm = "eig")
    res = lr_gpu.fit(X,y)

X = loan_norm_rdf
y = loan_rdf['default'].astype("float64")
run_times.add_result("linear_reg", "gpu", time_command(lr_gpu, repeat=5))



run_times.display_results()



# Cell
# Setup in notebook flag
try: from nbdev.imports import IN_NOTEBOOK
except: IN_NOTEBOOK=False


# Cell
# Imports
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')
import time
import timeit

from datetime import datetime
import math

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

import glob
import os
import sys
sys.path.append('../utils/')

#dask
import dask
from dask import dataframe as dd

# Rapids
import cudf
from cudf import DataFrame as RapidsDataFrame
# cudf                      0.11.0          cuda10.2_py37_673.g45906b8    https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda-early-access
# libcudf                   0.11.0          cuda10.2_657.g7f5e265    https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda-early-access
import torch


# Cell
# [print gpu dataframe] helper function to print GPU dataframes
def pgdf(gdf) :
    display(gdf.to_pandas())

# Cell
def time_command(cmd,repeat=1) :
    avg_runtime = timeit.timeit(cmd, number=repeat)
    return float(avg_runtime / repeat)

# Cell
# Dictionary to store results ..
# example "describe" : {"gpu" : []}
# TODO : make display results look better ..
class COMPARE() :
        ## Abstract Custom Implementations
    def __init__(self) :
        #nprint("Loading Data.  Overriding __init__ from dfutils")
        self.tests = []
        self.gpu_results = {}
        self.cpu_results = {}
        self.df_shape = (0,0)
        self.df_memory_gb = 0

    def add_result(self, test_name, gpu_result, runtime) :
        if test_name not in self.tests :
            self.tests.append(test_name)
            self.gpu_results[test_name] = []
            self.cpu_results[test_name] = []

        if(gpu_result == "gpu") :
            self.gpu_results[test_name].append(runtime)
        else :
            self.cpu_results[test_name].append(runtime)

    def display_results(self) :
        print("Dataframe size : {} {} GB".format(self.df_shape, self.df_memory_gb))
        print("{:<20} {:<20} {:<20} {:<20}".format("test", "CPU(s)", "GPU(s)", "GPU Speedup"))
        for i in self.tests :
            cpu_mean = sum(self.cpu_results[i]) / (len(self.cpu_results[i])+0.00001)
            gpu_mean = sum(self.gpu_results[i]) / (len(self.gpu_results[i])+0.00001)
            su = cpu_mean / (gpu_mean + .00001)
            print("{:<20} {:<20.4f} {:<20.4f} {:<20.2f}".format(i, cpu_mean, gpu_mean, su ))

run_times = COMPARE()


# Cell

filename = None
if not IN_NOTEBOOK:
    filename = "../Rapids/loan_project_df.parquet.gzip"
else :
    # import data
    filename = "./loan_project_df.parquet.gzip"

# Expand data to highlight performance difference
# 3 ~ 1GB dataset
# 4 ~ 2GB dataset
# ... etc
DATA_DOUBLE_FACTOR=3

# Pandas dataframe
loan_pdf = pd.read_parquet(filename)#  , names=ts_cols,dtype=ts_dtypes,skiprows=1)

# Rapids Dataframe
loan_rdf = cudf.read_parquet(filename)#  , names=ts_cols,dtype=ts_dtypes,skiprows=1)

# Cell
# Scale up data to 10 million rows
for i in range(DATA_DOUBLE_FACTOR) :
    loan_pdf = pd.concat([loan_pdf,loan_pdf],axis=0)
    loan_rdf = cudf.concat([loan_rdf,loan_rdf],axis=0)
    loan_rdf = loan_rdf.reset_index().drop("index",axis=1)
    loan_pdf = loan_pdf.reset_index().drop("index",axis=1)
    #pgdf(loan_rdf.head())
    #display(loan_pdf.head())


# Cell

# Dataframe attributes
print("Rapids")
print("\nDataframe datatypes\n---------------------")
print(loan_rdf.dtypes)
print("\nDataframe Shape (rows,cols)\n---------------------")
print(loan_rdf.shape)
print("\nDataframe dimensions\n---------------------")
print(loan_rdf.ndim)
print("\nDataframe Column names\n---------------------")
print(loan_rdf.columns)


# Cell

# Dataframe attributes
print("\n\nPandas")
print("\nDataframe datatypes\n---------------------")
print(loan_pdf.dtypes)
print("\nDataframe Shape (rows,cols)\n---------------------")
print(loan_pdf.shape)
print("\nDataframe dimensions\n---------------------")
print(loan_pdf.ndim)
print("\nDataframe Column names\n---------------------")
print(loan_pdf.columns)
print("\nDataframe Memory Usage\n---------------------")
print(loan_pdf.memory_usage(index=True).sum())


# Cell

run_times.df_shape = loan_pdf.shape
run_times.df_memory_gb = loan_pdf.memory_usage(index=True).sum() /10**9

print("Initial Data set size ~= {:0.3f} GB for this experiment".format(run_times.df_memory_gb))


# Cell

# Record results
def describe_gpu():
    loan_rdf.describe()

def describe_cpu():
    loan_pdf.describe()

#display(loan_rdf.describe().to_pandas())

run_times.add_result("describe", "gpu", time_command(describe_gpu))
run_times.add_result("describe", "cpu", time_command(describe_cpu))

run_times.display_results()

# Cell
# Record the results

def ohe_cpu() :
    x=pd.get_dummies(loan_pdf['grade'])

def ohe_gpu() :
    tmpdf = cudf.DataFrame()
    tmpdf['grade'] = loan_pdf['grade']
    x=ohe_gpu_df = cudf.get_dummies(tmpdf)

run_times.add_result("one_hot_encode", "cpu", time_command(ohe_cpu))
run_times.add_result("one_hot_encode", "gpu", time_command(ohe_gpu))
run_times.display_results()


# Cell
import datetime as dt

search_date = dt.datetime.strptime('2010-01-01', '%Y-%m-%d')

# Cell
# Filter Record results
def filter_cpu():
    loan_pdf.query('earliest_cr_line <= @search_date')

def filter_gpu():
    loan_rdf.query('earliest_cr_line <= @search_date')

run_times.add_result("filter_dt", "cpu", time_command(filter_cpu,repeat=3))
run_times.add_result("filter_dt", "gpu", time_command(filter_gpu,repeat=3))
run_times.display_results()


# Cell
# Sorting Record results
def sort_cpu():
    loan_pdf.sort_values(by=['fico_range_high','loan_amnt'],ascending=True)

def sort_gpu():
    loan_rdf.sort_values(by=['fico_range_high','loan_amnt'],ascending=True)

run_times.add_result("sorting", "cpu", time_command(sort_cpu, repeat=2))
run_times.add_result("sorting", "gpu", time_command(sort_gpu, repeat=2))
run_times.display_results()


# Cell
# custom function example : creates simple bins for loan_amount histogram
def roundto(num):
    roundto=5000
    a = int(num / roundto)
    return float(a*roundto)


# Cell
# Record the results
def hist_cpu() :
    loan_pdf['loan_bins'] = loan_pdf.loan_amnt.apply(roundto)
    loan_pdf['loan_bins'].value_counts()

def hist_gpu() :
    loan_rdf['loan_bins'] = loan_rdf.loan_amnt.applymap(roundto)
    loan_rdf['loan_bins'].value_counts()

run_times.add_result("histogram_ops", "cpu", time_command(hist_cpu,repeat=3))
run_times.add_result("histogram_ops", "gpu", time_command(hist_gpu,repeat=3))
run_times.display_results()


# Cell
# CPU / Pandas
# stats by grade
#grade_stats_pdf = loan_pdf.groupby('grade', as_index=False).agg({"annual_inc": ["count","mean"], "loan_amnt": ["count","mean"]})
grade_stats_pdf = loan_pdf.groupby('grade', as_index=False).agg({"annual_inc": "mean", "loan_amnt": "mean"})


# Cell
#GPU / Rapids
# stats by grade
# grade_stats_rdf = loan_rdf.groupby('grade', as_index=False).agg({"annual_inc": "count","annual_inc": "mean", "loan_amnt": "count","loan_amnt":"mean"})
grade_stats_rdf = loan_rdf.groupby('grade', as_index=False).agg({"annual_inc": "mean","loan_amnt":"mean"})


# Cell
# Record the results

def groupby_cpu() :
    loan_pdf.groupby('grade', as_index=False).agg({"annual_inc": "mean", "loan_amnt": "mean"})
# export

def groupby_gpu() :
    loan_rdf.groupby('grade', as_index=False).agg({"annual_inc": "mean","loan_amnt":"mean"})

run_times.add_result("groupby_ops", "cpu", time_command(groupby_cpu,repeat=4))
run_times.add_result("groupby_ops", "gpu", time_command(groupby_gpu,repeat=4))
run_times.display_results()

# Cell
# Record the results
def join_cpu() :
    loan_join_pdf = loan_pdf.set_index('grade').join(grade_stats_pdf.set_index('grade'),lsuffix='_l',rsuffix='_r',on="grade",how="left").reset_index()
def join_gpu() :
    loan_join_rdf = loan_rdf.set_index('grade').join(grade_stats_rdf.set_index('grade'),lsuffix='_l',rsuffix='_r',on="grade",how="left").reset_index()

run_times.add_result("join_ops", "cpu", time_command(join_cpu,repeat=1))
run_times.add_result("join_ops", "gpu", time_command(join_gpu,repeat=1))
run_times.display_results()

# Cell
# Helper function to normalize GPU dataframe function
def normalize_df(gdf) :
    for col in gdf.columns :
        gdf[col] = (gdf[col] - gdf[col].mean()) / gdf[col].std()
    return gdf

# Cell
X_cols = list(loan_rdf.columns)
print("Analysis Continuing with {}".format(X_cols))
X_cols.remove('default')
X_cols.remove('grade')
# X_cols.remove('grade_hash')
X_cols = [x for x in X_cols if loan_rdf[x].dtype == "float64" or loan_rdf[x].dtype == "int8"]
print("Analysis Continuing with {}".format(X_cols))
# All types must be same ....
for x in X_cols :
    loan_rdf[x] = loan_rdf[x].astype("float64")

#print(loan_rdf[X_cols].dtypes)
print("Normalizing dataframe prior to PCA")
loan_norm_rdf = normalize_df(loan_rdf[X_cols])
print("Copying dataframe to pandas")
loan_norm_pdf = loan_norm_rdf.to_pandas()


# Cell
# PCA
# Both import methods supported
from cuml import PCA
from cuml.decomposition import PCA as PCA_gpu
from sklearn.decomposition import PCA as PCA_cpu
n_components=5


# Cell
# record PCA performance results
def pca_cpu() :
    print("cpu pca")
    pca_loan_cpu = PCA_cpu(n_components=n_components)
    pca_loan_cpu.fit(loan_norm_pdf)


def pca_gpu() :
    print("gpu pca")
    pca_loan_gpu = PCA_gpu(n_components=n_components)
    pca_loan_gpu.fit(loan_norm_rdf)


#print(loan_norm_rdf.shape)
run_times.add_result("pca", "gpu", time_command(pca_gpu, repeat=2))
run_times.add_result("pca", "cpu", time_command(pca_cpu, repeat=2))

run_times.display_results()

# Cell
from sklearn.linear_model import LinearRegression as LRSKL
from cuml.linear_model import LinearRegression as LRCUML


# Cell
#Record Results

# CPU
def lr_cpu() :
    lr_cpu = LRSKL(fit_intercept = True, normalize = False)
    res = lr_cpu.fit(X,y)

X = loan_norm_rdf.to_pandas()
y = loan_rdf['default'].to_pandas()
run_times.add_result("linear_reg", "cpu", time_command(lr_cpu, repeat=5))


# GPU
def lr_gpu() :
    lr_gpu = LRCUML(fit_intercept = True, normalize = False, algorithm = "eig")
    res = lr_gpu.fit(X,y)

X = loan_norm_rdf
y = loan_rdf['default'].astype("float64")
run_times.add_result("linear_reg", "gpu", time_command(lr_gpu, repeat=5))



run_times.display_results()

