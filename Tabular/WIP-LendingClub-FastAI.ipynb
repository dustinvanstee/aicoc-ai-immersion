{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Finding Patterns in Data using IBM Power and PowerAI</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "In this lab we will explore an open source data set, and discover how we can use the tools that are part of **PowerAI** to explore and discover patterns in the data.  For this lab, we will make use of the Lending Club data set, **scikit learn, Tensorflow and Keras**.  Here is a brief description about Lending Club.\n",
    "\n",
    "```\n",
    "About the author's\n",
    "Dustin VanStee - Data Scientist\n",
    "Bob Chesebrough - Data Scientist\n",
    "IBM Systems AI Center of Competence\n",
    "contact : vanstee@us.ibm.com\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/techu-banner.png\" width=\"800\" height=\"500\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lending Club (LC)](https://www.lendingclub.com/) is the world’s largest online marketplace connecting borrowers and investors. It is transforming the banking system to make credit more affordable and investing more rewarding. Lending Club operates at a lower cost than traditional bank lending programs and pass the savings on to borrowers in the form of lower rates and to investors in the form of solid risk-adjusted returns.\n",
    "\n",
    "**The DATA**  \n",
    "The original data set is downloaded from [LC](https://www.lendingclub.com/info/download-data.action) covering complete loan data for all loans issued through the 2007-2018, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. Additional features include credit history, number of finance inquiries, address including zip codes, and state, and collections among others. It is quite rich and is an excellent example of credit risk data.  Interestingly, Goldman Sachs’ new peer-to-peer lending platform called Marcus was built almost entirely using the Lending Club data.\n",
    "\n",
    "Here is a link to some extra information regarding the fields of the data set.\n",
    "[Data Dictionary](https://github.com/dustinvanstee/mldl-101/blob/master/lab5-powerai-lc/LCDataDictionary.csv)\n",
    "\n",
    "**Important**\n",
    "\n",
    "In this notebook, we will play with the lending club data, conduct a set of exploratory analysis and try to apply various machine learning techniques to predict borrower’s default. We took a small sample of loans made in 2016 (130K) to help speed up the processing time for the lab\n",
    "\n",
    "\n",
    "Note : to remove a lot of the busy verbose code, we are making using of a utility python file called lc_utils.py.  For implemenation details you can refer here [python code](https://github.com/dustinvanstee/mldl-101/blob/master/lab5-powerai-lc/lc_utils.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick word on the data science method\n",
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/dsx-methodology.png\" width=\"900\" height=\"700\" align=\"middle\"/>\n",
    "\n",
    "Here we will use these simple high level steps to work through a typical data science problem.  This workflow is meant to be a high level guide, but in practice this is a highly iterative approach ...\n",
    "\n",
    "### Goals\n",
    "\n",
    "* Perform some initial analysis of the data for **Business Understanding**\n",
    "* **Prepare the Data** for our visualization and modeling\n",
    "* **Visualize** the data\n",
    "* Model using **Dimension Reduction** and **Classification** techniques\n",
    "* **Evaluate** the approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business/Data Understanding and Preparation\n",
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/techu-bu-dp.png\" width=\"800\" height=\"500\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment bootstrapping\n",
    "Run the following commands to install a few python packages for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q jupyter-pip\n",
    "# !pip install -q brunel\n",
    "# import brunel\n",
    "# !git fetch origin master\n",
    "# !git reset --hard origin/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code functions that are needed to run this lab\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import glob\n",
    "\n",
    "# custom library for some helper functions \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import myenv as myenv\n",
    "import brunel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils\") # go to parent dir\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lc_utils_2020 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "Here we load data that was previously downloaded from lendingclub.com.  For speed of this lab, we are restricting the number of loans ~ 130K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df = load_sample_data('acc')\n",
    "loan_df_orig = loan_df.copy()\n",
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samle the data to a reasonable number for debug\n",
    "fraction = 0.5 # sample all of it!\n",
    "loan_df = loan_df_orig.copy()\n",
    "if fraction < 1.0 :\n",
    "    loan_df =loan_df.sample(frac=0.1, replace=False, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics (1D)\n",
    "Lets look at some 1D and 2D descriptive statistics for this dataset\n",
    "\n",
    "In this dataset, we have all types of data.  Numerical, Categorical, Ranked data.  This small module will take you through what is typical done to quickly understand the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function provide the number of rows/cols\n",
    "# Information on the types of data\n",
    "# and a report of descriptive statistics\n",
    "\n",
    "quick_overview_1d_v2(loan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can get a quick assessment of the statistics for each column.  \n",
    "**Quick Question** can you answer what was the average income for the 133K loan applicants ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics (2D)\n",
    "Since we have over 100 numerical variables, creating a 2D correlation plot may be time consuming and difficult to interpret.  Lets look at correlations on a smaller scale for now....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab only a subset of columns\n",
    "cols = [\"loan_amnt\",\"annual_inc\",\"dti\",\"fico_range_high\",\"open_acc\",'funded_amnt', 'total_acc']\n",
    "quick_overview_2d(loan_df, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Question** : Can you find a negatively correlated variable to annual_inc in the chart above?  Can you think of a reason for this result ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Loan Default column.  This is the column we will predict later\n",
    "The **loan_status** column contains the information of whether or not the loan is in default. \n",
    "\n",
    "This column has more than just a 'default or paid' status.  Since our goal is to build a simple default classifier , we need to make a new column based off the **loan_status** column.\n",
    "\n",
    "Here we will look at all the categorical values in **loan_status**, and create a new column called **default** based off that one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create loan status .... \n",
    "# Todo insert some extra 'noise' here ...\n",
    "loan_df = create_loan_default(loan_df)\n",
    "loan_df.head(3) # scroll to the right, and see the new 'default' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation - Handle Null Values aka NaNs ...\n",
    "\n",
    "One part of the data science process thats especially time consuming is working with unclean data.  This lending club data set is a great example of that.  If you look at the dataframe shown above, you will see a number of columns with the indicator **NaN** .  This means 'not a number' and needs to be dealt with prior to any machine learning steps.  You have many options here.  Some options are listed below...\n",
    "\n",
    "* Fill with a value -> impute mean/median/min/max/other\n",
    "* drop rows with NaNs\n",
    "* drop columns with large number of NaNs \n",
    "* use data in other columns to derive\n",
    "\n",
    "All these methods are possible, but its up to the data scientist / domain expert to figure out the best approach.  There is definitely some grey area involved in whats the best approach.\n",
    "\n",
    "First, lets understand which columns have NaNs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every column, count the number of NaNs .... \n",
    "# code hint : uses df.isna().sum()\n",
    "\n",
    "#columns_with_nans(loan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, we have some work to do to clean up the NaN values.  Beyond NaN values, we also have to transform columns if they aren't formatted correctly, or maybe we want to transform a column based on custom requirements.  \n",
    "\n",
    "```\n",
    "Example : column=employee_length , values=[1,2,3,4,5,6,7,8,9,10+] formatted as a string\n",
    "          transform into \n",
    "          column=employee_length, [0_3yrs,4_6yrs,gt_6yrs] (categorical:strings)\n",
    "```\n",
    "          \n",
    "Luckily, we took care to process and clean this data below using a few functions.  In practice, **this is where data scientists spend a large portion of their time** as this requires detailed domain knowledge to clean the data.  We have made a fair number of assumptions about how to process the data which we won't go into due to time contraints for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD FLOW ....\n",
    "# The following cleaning of the data makes use of the steps shown below.....\n",
    "\n",
    "#loan_df1 = drop_sparse_numeric_columns(loan_df)\n",
    "#loan_df2 = drop_columns(loan_df1)\n",
    "#loan_df3 = impute_columns(loan_df2)\n",
    "#loan_df4 = handle_employee_length(loan_df3)\n",
    "#loan_df5 = handle_revol_util(loan_df4)\n",
    "#loan_df6 = drop_rows(loan_df5)\n",
    "\n",
    "#loan_df = clean_lendingclub_data(loan_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTAI FLOW\n",
    "\n",
    "#loan_df1 = drop_sparse_numeric_columns(loan_df)\n",
    "loan_df1 = drop_columns(loan_df)\n",
    "#loan_df3 = impute_columns(loan_df2)\n",
    "loan_df2 = handle_employee_length(loan_df1)\n",
    "loan_df3 = handle_revol_util(loan_df2)\n",
    "#loan_df6 = drop_rows(loan_df5)\n",
    "\n",
    "#loan_df = clean_lendingclub_data(loan_df)\n",
    "loan_df = loan_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Sanity check ....\n",
    "# If we did our job right, there should not be any NaN's left.  \n",
    "# Use this convenience function to check\n",
    "\n",
    "# code hint df.isna().sum()\n",
    "\n",
    "#columns_with_nans(loan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation - Handle Time Objects\n",
    "Sometimes for columns that contain date information, you may want to break them down into individual columns like month, day, day of week etc.  For our use case, we will create a new column called `time_history` that will indicate how long an applicant has been a borrower.  This is an example of **feature engineering**.  Essentially, using business logic to create a new column (feature) that may have predictive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df = create_time_features(loan_df)\n",
    "loan_df.earliest_cr_line = pd.to_datetime(loan_df.earliest_cr_line, errors='coerce')\n",
    "loan_df.issue_d = pd.to_datetime(loan_df.issue_d, errors='coerce')\n",
    "loan_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical Data to One hot encode ###\n",
    "\n",
    "If you look above at the data frame, we are almost ready to start building models.  However, there is one important step to complete.  Notice we have some columns that are still built out of string data \n",
    "```\n",
    "example column=home_ownership values=[RENT, MORTGAGE, OWN]\n",
    "```\n",
    "Machine learning algorithms only process numerical data, so we need to transform these **categorical columns** into **indicator columns**\n",
    "\n",
    "From the example above, the transform would yield 3 new columns\n",
    "\n",
    "```\n",
    "example column=RENT values=[0,1]\n",
    "        column=MORTGAGE values=[0,1]\n",
    "        column=OWN values=[0,1]\n",
    "```\n",
    "\n",
    "Conveniently pandas has a nice function called **get_dummies** that we will use for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip for fastAI\n",
    "# Transform categorical data into binary indicator columns\n",
    "# code hint, uses pd.get_dummies\n",
    "\n",
    "# loan_df = one_hot_encode_keep_cols(loan_df)\n",
    "loan_df.head() # once complete, see how many new columns you have!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result after data preparation ....\n",
    "\n",
    "Ok, so you made it here, lets take a look at the final results of your data preparation work.  It may be helpful to  **qualitatively compare** your original data frame to this one and see how different they look..  Execute the cells below to get a sense of what the tranformations accomplished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df_orig.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Train / Test Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(loan_df, test_size=0.20, random_state=52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Data For H20 or other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE=\"020320\"\n",
    "train_df.to_csv(path_or_buf=\"../curateddata/lc_h2o_train_{}.csv\".format(DATE),index=False,header=True)\n",
    "test_df.to_csv(path_or_buf=\"../curateddata/lc_h2o_test_{}.csv\".format(DATE),index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "As you saw, when you 'describe' a data frame, you get a table statistics showing you the mean,min,max and other statistics about each column.  This is good, but sometimes its also good to look at the histograms of the data as well.  Lets Visualize some of the distributions from our dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/data-visualization.png\" width=\"800\" height=\"500\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot distribution charts for all the numerical columns in our dataframe\n",
    "plot_histograms(loan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brunel Visualization Examples\n",
    "Here we use the builtin Brunel Visualization graphics package.  This documentation was useful in the preparation of the following graphs.\n",
    "* https://brunel.mybluemix.net/docs/Brunel%20Documentation.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a statistics data frame based on issue date\n",
    "# aggregate on loan amount\n",
    "vis_df = loan_df.copy()\n",
    "vis_df['default'] = loan_df['default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome Variable: Loan Status\n",
    "On the left is the breakdown of all loan status classifications.  On the right is our simple default classification based on our data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=vis_df.sample(5000) # downsample for speed\n",
    "%brunel data('a') bar x(loan_status) y(#count:linear) color(loan_status)  percent(#count:overall) tooltip(#all) | stack polar bar y(#count) color(default) percent(#count) tooltip(#all) :: width=1200, height=350 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf=vis_df.sample(5000) # downsample for speed\n",
    "\n",
    "figure, axes = plt.subplots(nrows=2, ncols=2)\n",
    "#ax.plot(kind='pie', subplots=True, figsize=(16,8))\n",
    "ax1.pie(ldf['default'],  autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loan Purpose\n",
    "Lets try to get a sense of why people are borrowing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_count = vis_df.groupby('purpose')['loan_status'].count().to_frame().rename(columns = {'loan_status':'count'})\n",
    "%brunel bubble data('purpose_count') color(COUNT:[blues, reds]) size(COUNT) label(PURPOSE) tooltip(#all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this could go on forever, but hopefully you get a sense of the power of data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/modeling.png\" width=\"800\" height=\"500\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastAI Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular import *\n",
    "#loan_df.dtypes\n",
    "\n",
    "fai_df = train_df[:10000].copy()\n",
    "fai_df = fai_df[fai_df.loan_amnt.isnull()==False]\n",
    "\n",
    "add_datepart(fai_df, 'earliest_cr_line',prefix=\"ecl_\",time=True) # inplace\n",
    "add_datepart(fai_df, 'issue_d',prefix=\"iss_\",time=True) # inplace\n",
    "\n",
    "#fai_df.describe()\n",
    "fai_df.dtypes\n",
    "display(fai_df.head(5))\n",
    "print(\"Fast AI num records = {}\".format(len(fai_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df.select_dtypes(include=['object']).columns.values)\n",
    "quick_overview_1d_v2(fai_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean out NaNs but leave categorical !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Transformers and Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = [FillMissing, Categorify, Normalize]\n",
    "# Target / Label Column\n",
    "dep_var   = 'default'\n",
    "\n",
    "# Categorical Variables\n",
    "cat_names = list(fai_df.select_dtypes(include=['object','bool','int64']).columns.values)\n",
    "cat_names.remove('id')\n",
    "cat_names.remove('default')\n",
    "\n",
    "print(\"Total number of categorical columns :{}\".format(len(cat_names)))\n",
    "cat_names = cat_names[0:10]\n",
    "#cat_names = cat_names[0:4] + cat_names[6:10]\n",
    "\n",
    "#Continuous Variables\n",
    "cont_names = list(fai_df.select_dtypes(include=['float64']).columns.values)\n",
    "cont_names.remove('member_id')\n",
    "\n",
    "print(\"Total number of continuous columns :{}\".format(len(cont_names)))\n",
    "cont_names = cont_names[0:50]\n",
    "#print(type)\n",
    "#fastai_cols = cat_names + cont_names\n",
    "# \n",
    "print(\"\\nCategoricals ({}): {},{}\".format(len(cat_names),cat_names,type(cat_names)))\n",
    "print(\"\\nContinuous ({}): {}\".format(len(cont_names),cont_names))\n",
    "#print(\"\\nfastai_cols : {}\".format(fastai_cols))\n",
    "\n",
    "# Setup Split\n",
    "path= \"\"\n",
    "split = int(len(fai_df)*0.30)\n",
    "valid_idx = range(len(fai_df)-split, len(fai_df))\n",
    "print(\"\\nIndex splits training : 0:{}\".format(len(fai_df)-split))\n",
    "print(\"Index splits validation : {}\".format(valid_idx))\n",
    "print('\\n')\n",
    "\n",
    "fai_df2 = fai_df[cat_names+cont_names+[dep_var]].copy().reset_index()\n",
    "columns_with_nans(  fai_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Tabular Databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of categorical columns :{}\".format(len(cat_names)))\n",
    "print(\"Total number of continuous columns :{}\".format(len(cont_names)))\n",
    "print(\"Total number of continuous columns :{}\".format(len(fai_df2.columns)))\n",
    "\n",
    "data = TabularDataBunch.from_df(path=\"\",df=fai_df2, \n",
    "        dep_var=dep_var, procs=procs, valid_idx=valid_idx,\n",
    "        cat_names=cat_names, cont_names=cont_names)\n",
    "#data.train_ds.x.inner_df.size\n",
    "#print(data.train_ds.cat_names)  # `cont_names` defaults to: set(df)-set(cat_names)-{dep_var}\n",
    "#print(data.train_ds.cont_names)  # `cont_names` defaults to: set(df)-set(cat_names)-{dep_var}\n",
    "\n",
    "#      1 data = (TabularList.from_df(df, path=PATH, cat_names=cat_names, procs=procs)\n",
    "#      2                            .random_split_by_pct()\n",
    "#----> 3                            .label_from_df(cols=dep_var)\n",
    "#      4                            .add_test(test)\n",
    "#      5                            .databunch())\n",
    "#data.train_ds.x.inner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.train_ds.x.inner_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(data)\n",
    "#'add_test','add_tfm','batch_size','create','device','dl','dl_tfms','dls',\n",
    "#'empty_val','export','fix_dl','fix_ds','from_df','is_empty','label_list',\n",
    "#'load_empty','loss_func','one_batch','one_item','path','remove_tfm','sanity_check',\n",
    "#'save','show_batch','single_dl','single_ds',\n",
    "#'test_dl','test_ds','train_dl','train_ds','valid_dl','valid_ds']\n",
    "\n",
    "# dir(data.train_ds)  # Data Set, _dl is data_loader\n",
    "# 'c','databunch','export','filter_by_func','get_state','item',\n",
    "# 'load_empty','load_state','new','predict','process',\n",
    "# 'set_item','tfm_y','tfmargs','tfmargs_y','tfms','tfms_y',\n",
    "# 'to_csv','to_df','transform','transform_y'\n",
    "\n",
    "# data.train_ds.c\n",
    "# data.train_ds.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# for i in range(35000) :\n",
    "#     a=str(data.train_ds.get(i))\n",
    "#     if re.search(\"id #na\",a) :\n",
    "#         print(str(a))\n",
    "# for i in range(15000) :\n",
    "#     a=str(data.valid_ds.get(i))\n",
    "#     if re.search(\"id #na\",a) :\n",
    "#         print(str(a))\n",
    "# # ALL valids have #na# !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df.iloc[35000:35002])\n",
    "# print(data.train_ds.get(0))\n",
    "# print()\n",
    "# print(data.valid_ds.get(0))\n",
    "# print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Tabular Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(data, layers=[200,100], emb_drop=0.2, emb_szs={'addr_state': 2,'zip_code':11}, metrics=accuracy,)\n",
    "#learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(start_lr=1e-8,end_lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(div_factor=100,max_lr=5e-3,cyc_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.predict(df.iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn methods\n",
    "# 'add_time', 'apply_dropout', 'backward', 'bn_wd', 'callback_fns', 'callbacks', \n",
    "# 'clip_grad', 'create_opt', 'data', 'destroy', 'dl', 'export', \n",
    "# 'fit', 'fit_fc', 'fit_one_cycle', 'freeze', 'freeze_to', 'get_preds', \n",
    "# 'init', 'interpret', 'layer_groups', 'load', 'loss_func', \n",
    "# 'lr_find', 'lr_finder', 'lr_range', 'metrics', 'mixup', \n",
    "# 'model', 'model_dir', 'one_cycle_scheduler', 'opt', 'opt_func', \n",
    "# 'path', 'pred_batch', 'predict', 'predict_with_mc_dropout', \n",
    "# 'purge', 'recorder', 'save', 'show_results', 'silent', 'split', \n",
    "# 'summary', 'to_fp16', 'to_fp32', 'train_bn', 'true_wd', 'unfreeze', \n",
    "# 'validate', 'wd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = learn.interpret()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interp.confusion_matrix()\n",
    "interp.plot_confusion_matrix()\n",
    "#interp.plot_tab_top_losses(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test set creation\n",
    "\n",
    "One of the key points in any machine learning workflow is the **partitioning** of the data set into **train** and **test** sets.  The key idea here is that a model is built using the training data, and evaluated using the test data.  \n",
    "\n",
    "There are more nuances to how you partition data into train/test sets, but for purposes of this lab we will omit these finer points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lc_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lendingclub_ml object that will hold our test, and contain methods used for testing.\n",
    "# Implementation done like this to ease the burden on users for keeping track of train/test sets for different\n",
    "# models we are going to build.\n",
    "\n",
    "my_analysis = lendingclub_ml(loan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train / test split of your data set.  Paramter is test set size percentage \n",
    "# Returns data in the form of dataframes\n",
    "\n",
    "my_analysis.create_train_test(test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Credits \n",
    "* Bob Chesebrough - IBM CSSC Data Scientist\n",
    "* Catherine Cao - IBM FSS Data Scientist\n",
    "* [Hands on Machine Learning - Geron] (https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)\n",
    "\n",
    "### More Learning\n",
    "* Coursera Deeplearning.ai  (Ng)\n",
    "* Coursera Machine Learning (Ng)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "354.267px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
