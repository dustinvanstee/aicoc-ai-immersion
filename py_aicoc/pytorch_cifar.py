# AUTOGENERATED! DO NOT EDIT! File to edit: Pytorch-Classification/Pytorch_using_CIFAR10.ipynb (unless otherwise specified).

__all__ = ['xt', 'xt_test', 'train_set', 'test_set', 'batch_size', 'train_loader', 'test_loader', 'labels_map',
           'NetCNN3L', 'model_summary', 'weights_uniform_random', 'weights_xavier', 'train_test', 'curves',
           'reset_weights', 'model', 'use_cuda', 'optimizer', 'criterion', 'TRAINING_EPOCHS', 'resnet18', 'xfer_model',
           'data_transforms', 'batch_size', 'train_loader', 'test_loader', 'reset_weights', 'use_cuda', 'optimizer',
           'criterion', 'curves', 'TRAINING_EPOCHS']

# Cell
# Setup in notebook flag
import sys
try: from nbdev.imports import IN_NOTEBOOK
except: IN_NOTEBOOK=False

if IN_NOTEBOOK :
    print("In Notebooke mode")
    # Adding python utils library directory
    sys.path.append("../py_aicoc")
else :
    print("Running in batch mode")


# Cell
# Some good references
# https://github.com/zalandoresearch/fashion-mnist
# https://pytorch.org/docs/stable/torchvision/datasets.html#mnist
# http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/#  <- performance results!

import torch
import numpy as np
import torch.optim as optim
from torch.nn.init import xavier_normal_ , uniform_
from torchvision.utils import make_grid
# import utils   # custom functions for this notebook
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.datasets as dset
import torchvision.transforms as transforms
import cifar_utils
import os
# Write checkpoint data to directory where this notebook is running..
os.environ['TORCH_HOME'] = '.' #setting the environment variable



# Cell
# Grab the data
xt = transforms.Compose([
     transforms.RandomRotation(30, resample=False, expand=False, center=None),
     transforms.ToTensor(),
])

xt_test = transforms.Compose([
     transforms.ToTensor(),
])

#torchvision.datasets.CIFAR10(root, train=True, transform=None, target_transform=None, download=False)

#transform=transforms.ToTensor()
train_set = dset.CIFAR10(root = './cifar10_data',
                                 download=True, transform=xt)

test_set = dset.CIFAR10(root='./cifar10_data', train=False,
                         download=True, transform=xt_test) #transform=trans



# Cell
batch_size = 64

# https://pytorch.org/docs/stable/data.html
train_loader = torch.utils.data.DataLoader(
                 dataset=train_set,
                 batch_size=batch_size,
                 shuffle=True)

test_loader = torch.utils.data.DataLoader(
                dataset=test_set,
                batch_size=batch_size,
                shuffle=False)


print('Number of Training samples: ', len(train_set))
print('Number of Test samples: ',     len(test_set))
print('Batch Size : ',                batch_size)
print('==>>> total training batches : {}'.format(len(train_loader)))
print('==>>> total testing batches : {}'.format(len(test_loader)))
#print('type {}'.format(type(train_fm_loader)))


# Cell
labels_map = {
    0:"plane",
    1:"car",
    2:"bird",
    3:"cat",
    4:"deer",
    5:"dog",
    6:"frog",
    7:"horse",
    8:"ship",
    9:"truck"
}


# Cell
class NetCNN3L(nn.Module):

    def __init__(self):
        super(NetCNN3L, self).__init__()
        self.name = "NetCNN3L"
        # output dimension (H,W) -> H - kernel + 1 - 2p
        # in_NCHW=[Nx3x32x32 image], 3x3 square kernel, out_NCHW=[Nx32x32x32 image]
        # cin=3,cout=32
        self.conv1_1 = nn.Conv2d(3, 32, kernel_size=(3,3),padding=(1,1))  # same padding

        # in_NCHW=[Nx32x32x32 image], 3x3 square kernel, out_NCHW=[Nx32x32x32 image]
        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=(3,3),padding=(1,1))# same padding

        # max pool here, shrinks 32x32 - > 16x16 image

        # in_NCHW=[Nx32x16x16 image], 3x3 square kernel, out_NCHW=[Nx64x16x16 image]
        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=(3,3),padding=(1,1))

        # in_NCHW=[Nx64x16x16 image], 3x3 square kernel, out_NCHW=[Nx64x16x16 image]
        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=(3,3),padding=(1,1))
        # max pool here, shrinks 16x16 - > 8x8 image

        # an affine operation: y = Wx + b
        # 64 x 8 x 8
        self.fc1 = nn.Linear(4096, 512)
        self.fc2 = nn.Linear(512, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = self.conv1_1(x); self.c1_1 = x # (for plotting)
        x = F.relu(x)
        x = self.conv1_2(x); self.c1_2 = x # (for plotting)
        x = F.relu(x)
        x = F.max_pool2d(x, (2,2))
        x = F.dropout(x, p=0.25)
        x = self.conv2_1(x); self.c2_2 = x # (for plotting)
        x = F.relu(x)
        x = self.conv2_2(x); self.c2_2 = x # (for plotting)
        x = F.relu(x)
        x = F.max_pool2d(x, (2,2))
        x = F.dropout(x, p=0.25)
        #Flatten x for fully connected layer
        x = x.view(-1, 4096)
        #print(x.size())
        x = self.fc1(x)
        #print(x.size())
        x = F.relu(x)
        x = F.dropout(x, p=0.5)
        x = self.fc2(x)
        x = F.softmax(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features



# Cell
# Lets print out the Summary of our layers !!

def model_summary(net) :
    #print(net.children)
    header = "{:<30}{:<30}{:<20}".format("Layer" ,"Weight Size", "#Params")
    print(header)
    print("="*70)
    tp = 0
    for (ln,i) in net.named_parameters() :
        #print(ln, i.size(),np.prod(np.asarray(i.size())))
        trainable_params = np.prod(np.asarray(i.size()))
        ln_out = "{:<30}{:<30}{:<20}".format(ln , str(i.size()), trainable_params)
        print(ln_out)
        tp += trainable_params
    print("="*70)
    print("Total params: {}".format(tp))

model_summary(NetCNN3L())


# Cell
def weights_uniform_random(m):
    if isinstance(m, nn.Conv2d):
        uniform_(m.weight.data)
    elif isinstance(m, nn.Linear) :
        uniform_(m.weight.data)
        m.bias.data.fill_(0)

# Initialize with Xavier normal distribuition
def weights_xavier(m):
    if isinstance(m, nn.Conv2d):
        xavier_normal_(m.weight.data)
    elif isinstance(m, nn.Linear) :
        xavier_normal_(m.weight.data)
        m.bias.data.fill_(0)


# Cell
def train_test(model, epoch, data_loader, batch_size, use_cuda, mode='train', debug=False):
    # training
    cm_loss = 0
    loss = 0
    cm_correct = cm_tot = running_acc = 0
    Y_onehot = torch.FloatTensor(batch_size, 10)

    if(mode == 'test') :
        model.eval()
    else :
        model.train()

    print("model.training mode = {}".format(model.training))

    for i,(X,Y) in enumerate(data_loader):
        if use_cuda :
            X = X.to('cuda')
            Y = Y.to('cuda')
            Y_onehot = Y_onehot.to('cuda')

        if(len(Y) == batch_size) :
            # viz_network(epoch, i,loss,running_acc,net)
            optimizer.zero_grad()   # zero the gradient buffers
            yhat = model(X)
            #print(print(net.conv1.parameters))
            #parameters = net.conv1.parameters.flatten().detach().numpy()
            # In your for loop

            Y_onehot.zero_()
            #print(Y_onehot.size(),Y.size())
            Y_onehot.scatter_(1, Y.unsqueeze(1), 1)

            if(debug) :
                print("Labels")
                print(Y_onehot.size())
                print(Y_onehot)
                print("Predicted")
                print(yhat.size())
                print(yhat)

            #print(yhat.type(),Y_onehot.type())
            cm_correct += cifar_utils.accuracy(yhat,Y)
            cm_tot += batch_size
            running_acc = cm_correct/cm_tot

            # MSE version loss = criterion(yhat, Y_onehot.float())
            # Crossentropy version
            loss = criterion(yhat, Y)

            cm_loss += np.float(loss)
            if(mode == 'train') :
                loss.backward()
                optimizer.step()    # Does the update

                #print(model.conv1_1.weight.grad)

        print("\rEpoch {} : {} out of 50000".format(epoch, i*batch_size), end =" ")
        #print('\r', 'Iteration', i, 'Score:', uniform(0, 1), end='')

    cm_loss = cm_loss/cm_tot
    return (cm_loss,cm_correct,cm_tot)


# Cell
curves = {'train' : [], 'test' : []}

# Cell
# reload_libs()
# Training run here ...

print("****************************************")
print("*  Custom Model Example                *")
print("****************************************")



reset_weights=True

# Instantiate Model
model = NetCNN3L()
# Enable GPU's
use_cuda = True
if use_cuda and reset_weights:
    model = NetCNN3L().to("cuda")

# create your optimizer
optimizer = optim.Adam(model.parameters(), lr=0.0001)
criterion = nn.CrossEntropyLoss()

if(reset_weights) :
    #model.apply(weights_uniform_random)  # comment out this line for round 2
    model.apply(weights_xavier)         # uncomment this line for round 2

# in your training loop:
if(reset_weights) :
    curves = {'train' : [], 'test' : []}

TRAINING_EPOCHS=2
if IN_NOTEBOOK : cifar_utils.viz_network(0,curves,model,use_cuda)

for epoch in range(TRAINING_EPOCHS) :

    train_res = train_test(model, epoch, train_loader, batch_size, use_cuda, mode='train')
    curves['train'].append(train_res)

    if IN_NOTEBOOK : cifar_utils.viz_network(epoch,curves,model,use_cuda)

    test_res = train_test(model, epoch, test_loader,batch_size, use_cuda, mode='test')
    curves['test'].append(test_res)

    #print(curves)
    print("**")
    print("Epoch:{} / Train loss: {} / Train Accuracy:{:.02f}".format(epoch, train_res,
                                                    100*float(train_res[1])/float(train_res[2])))
    print("Epoch:{} / Test loss : {} / Test  Accuracy:{:.02f}".format(epoch, test_res,
                                                    100*float(test_res[1])/float(test_res[2])))


# Cell
import torchvision.models as models
resnet18 = models.resnet18(pretrained=True)

xfer_model = resnet18


# Cell
## Drop last layer and bolt on my final layer
for param in xfer_model.parameters():
    param.requires_grad = True
    # Replace the last fully-connected layer
    # Parameters of newly constructed modules have requires_grad=True by default

xfer_model.fc = nn.Linear(512, 10) # assuming that the fc7 layer has 512 neurons, otherwise change it
xfer_model.cuda()
model_summary(xfer_model)
#for param in xfer_model.parameters():
#    print(param.requires_grad)


# Cell
# Data augmentation and normalization for training
# Just normalization for validation
# see https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html

data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

# Cell
batch_size = 32

# https://pytorch.org/docs/stable/data.html
train_loader = torch.utils.data.DataLoader(
                 dataset=train_set,
                 batch_size=batch_size,
                 shuffle=True)

test_loader = torch.utils.data.DataLoader(
                dataset=test_set,
                batch_size=batch_size,
                shuffle=False)


print('Number of Training samples: ', len(train_set))
print('Number of Test samples: ',     len(test_set))
print('Batch Size : ',                batch_size)
print('==>>> total training batches : {}'.format(len(train_loader)))
print('==>>> total testing batches : {}'.format(len(test_loader)))
#print('type {}'.format(type(train_fm_loader)))

# Cell
# reload_libs()
# Training run here ...
print("****************************************")
print("*  Transfer Learning Example           *")
print("****************************************")

reset_weights=False

# Instantiate Model

# Enable GPU's
use_cuda = True
if use_cuda and reset_weights:
    xfer_model = xfer_model.to("cuda")

# create your optimizer
optimizer = optim.Adam(xfer_model.parameters(), lr=0.0001)
criterion = nn.CrossEntropyLoss()

if(reset_weights) :
    xfer_model.apply(weights_xavier)  # comment out this line for round 2
    #model.apply(weights_xavier)         # uncomment this line for round 2

# in your training loop:

curves = {'train' : [], 'test' : []}
TRAINING_EPOCHS=2

if IN_NOTEBOOK : cifar_utils.viz_network(0,curves,xfer_model,use_cuda)

for epoch in range(TRAINING_EPOCHS) :

    train_res = train_test(xfer_model, epoch, train_loader, batch_size, use_cuda, mode='train',debug=False)
    curves['train'].append(train_res)

    if IN_NOTEBOOK : cifar_utils.viz_network(epoch,curves,xfer_model,use_cuda)

    test_res = train_test(xfer_model, epoch, test_loader,batch_size, use_cuda, mode='test',debug=False)
    curves['test'].append(test_res)

    #print(curves)
    #print("Epoch {} Train loss = {}".format(epoch, train_res))
    #print("Epoch {} Test loss = {}".format(epoch, test_res))

    print("**")
    print("Epoch:{} / Train loss: {} / Train Accuracy:{:.02f}".format(epoch, train_res,
                                                        100*float(train_res[1])/float(train_res[2])))
    print("Epoch:{} / Test loss : {} / Test  Accuracy:{:.02f}".format(epoch, test_res,
                                                        100*float(test_res[1])/float(test_res[2])))
